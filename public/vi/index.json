[
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/",
	"title": "Workshop T·ªëi ∆∞u h√≥a Chi ph√≠ EMR",
	"tags": [],
	"description": "",
	"content": "Workshop T·ªëi ∆∞u h√≥a Chi ph√≠ EMR Ti·∫øt ki·ªám ƒë·∫øn 70% chi ph√≠ Amazon EMR Ch√†o m·ª´ng ƒë·∫øn v·ªõi Workshop t·ªëi ∆∞u h√≥a chi ph√≠ Amazon EMR to√†n di·ªán! H·ªçc c√°c chi·∫øn l∆∞·ª£c ƒë√£ ƒë∆∞·ª£c ch·ª©ng minh ƒë·ªÉ gi·∫£m ƒë√°ng k·ªÉ chi ph√≠ EMR trong khi v·∫´n duy tr√¨ hi·ªáu su·∫•t v√† ƒë·ªô tin c·∫≠y.\nüéØ M·ª•c ti√™u Workshop Sau khi ho√†n th√†nh workshop n√†y, b·∫°n s·∫Ω:\nGi·∫£m chi ph√≠ EMR 60-70% b·∫±ng spot instances v√† auto scaling Tri·ªÉn khai gi√°m s√°t ch·ªß ƒë·ªông v·ªõi c·∫£nh b√°o v√† kh·∫Øc ph·ª•c t·ª± ƒë·ªông Th√†nh th·∫°o c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u chi ph√≠ production-ready Tri·ªÉn khai gi√°m s√°t v√† qu·∫£n tr·ªã c·∫•p doanh nghi·ªáp üí∞ Ti·∫øt ki·ªám Chi ph√≠ D·ª± ki·∫øn Quy m√¥ C√¥ng ty Tr∆∞·ªõc Sau Ti·∫øt ki·ªám/Th√°ng Nh·ªè $2,000 $600 $1,400 Trung b√¨nh $10,000 $3,000 $7,000 L·ªõn $40,000 $12,000 $28,000 üìö C·∫•u tr√∫c Workshop Ph·∫ßn 1: Spot Instances (Ti·∫øt ki·ªám 39%) Mixed instance groups X·ª≠ l√Ω interruption Best practices Ph·∫ßn 2: Auto Scaling (Ti·∫øt ki·ªám th√™m 40%) Managed scaling policies Custom metrics T·ªëi ∆∞u hi·ªáu su·∫•t Ph·∫ßn 3: Gi√°m s√°t \u0026amp; C·∫£nh b√°o (Ti·∫øt ki·ªám th√™m 15%) Theo d√µi chi ph√≠ th·ªùi gian th·ª±c Kh·∫Øc ph·ª•c t·ª± ƒë·ªông Dashboard production Ph·∫ßn 4: D·ªçn d·∫πp T√†i nguy√™n Quy tr√¨nh d·ªçn d·∫πp ho√†n ch·ªânh X√°c minh chi ph√≠ Best practices üéì ƒê·ªëi t∆∞·ª£ng M·ª•c ti√™u Ch√≠nh:\nData Engineers (2+ nƒÉm kinh nghi·ªám) DevOps Engineers (c√≥ ki·∫øn th·ª©c AWS) Solutions Architects (t·∫≠p trung t·ªëi ∆∞u chi ph√≠) Ph·ª•:\nSenior Developers (kinh nghi·ªám big data) Cloud Engineers (kinh nghi·ªám EMR) Technical Leads (qu·∫£n l√Ω chi ph√≠) ‚è±Ô∏è Th·ªùi gian T·ªïng th·ªùi gian: 4-6 gi·ªù\nPh·∫ßn 1: 1.5 gi·ªù Ph·∫ßn 2: 2 gi·ªù Ph·∫ßn 3: 2.5 gi·ªù Ph·∫ßn 4: 30 ph√∫t üìã Y√™u c·∫ßu Ti√™n quy·∫øt Ki·∫øn th·ª©c B·∫Øt bu·ªôc ‚úÖ AWS c∆° b·∫£n (EC2, S3, IAM) ‚úÖ EMR/Spark cƒÉn b·∫£n ‚úÖ Kinh nghi·ªám command line ‚úÖ Ki·∫øn th·ª©c Python c∆° b·∫£n Quy·ªÅn truy c·∫≠p B·∫Øt bu·ªôc ‚úÖ AWS account v·ªõi quy·ªÅn EMR ‚úÖ AWS CLI ƒë√£ c·∫•u h√¨nh ‚úÖ Ng√¢n s√°ch $50-100 cho hands-on labs C√¥ng c·ª• Khuy·∫øn ngh·ªã ‚úÖ AWS CLI v2 ‚úÖ Python 3.7+ ‚úÖ Text editor/IDE ‚úÖ Web browser üöÄ B·∫Øt ƒë·∫ßu Thi·∫øt l·∫≠p Y√™u c·∫ßu - C·∫•u h√¨nh m√¥i tr∆∞·ªùng Ph·∫ßn 1: Spot Instances - Tri·ªÉn khai ti·∫øt ki·ªám 39% Ph·∫ßn 2: Auto Scaling - Th√™m 40% ti·∫øt ki·ªám Ph·∫ßn 3: Gi√°m s√°t - Kh·∫£ nƒÉng quan s√°t ho√†n ch·ªânh Ph·∫ßn 4: D·ªçn d·∫πp - D·ªçn d·∫πp t√†i nguy√™n üèÜ Ch·ªâ s·ªë Th√†nh c√¥ng Sau khi ho√†n th√†nh workshop:\nGi·∫£m chi ph√≠ 70% ƒë·∫°t ƒë∆∞·ª£c 99.9% uptime ƒë∆∞·ª£c duy tr√¨ 80% √≠t can thi·ªáp th·ªß c√¥ng h∆°n Gi√°m s√°t production-ready ƒë∆∞·ª£c tri·ªÉn khai üí° C√¥ng ngh·ªá Ch√≠nh Amazon EMR - Managed Hadoop/Spark EC2 Spot Instances - Ti·∫øt ki·ªám ƒë·∫øn 90% Auto Scaling - Qu·∫£n l√Ω capacity ƒë·ªông CloudWatch - Gi√°m s√°t v√† c·∫£nh b√°o Lambda - Kh·∫Øc ph·ª•c t·ª± ƒë·ªông SNS - Th√¥ng b√°o c·∫£nh b√°o üéâ Nh·ªØng g√¨ b·∫°n s·∫Ω X√¢y d·ª±ng H·ªá th·ªëng t·ªëi ∆∞u h√≥a chi ph√≠ EMR ho√†n ch·ªânh, production-ready bao g·ªìm:\nQu·∫£n l√Ω spot instance th√¥ng minh Ch√≠nh s√°ch auto scaling ƒë·ªông Gi√°m s√°t chi ph√≠ th·ªùi gian th·ª±c Kh·∫Øc ph·ª•c t·ª± ƒë·ªông Dashboard ƒëi·ªÅu h√†nh M·∫πo chuy√™n nghi·ªáp: Workshop n√†y t·ª± ho√†n v·ªën ch·ªâ b·∫±ng vi·ªác ngƒÉn ch·∫∑n m·ªôt l·∫ßn v∆∞·ª£t chi ph√≠!\nS·∫µn s√†ng ti·∫øt ki·ªám h√†ng ngh√¨n ƒë√¥ la cho chi ph√≠ EMR? H√£y b·∫Øt ƒë·∫ßu!\n"
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/1-spot-instances/",
	"title": "Ph·∫ßn 1: Spot Instances",
	"tags": [],
	"description": "",
	"content": "Ph·∫ßn 1: S·ª≠ d·ª•ng Spot Instances ƒë·ªÉ gi·∫£m chi ph√≠ Trong ph·∫ßn n√†y, b·∫°n s·∫Ω h·ªçc c√°ch t·∫°o EMR cluster s·ª≠ d·ª•ng Spot Instances ƒë·ªÉ gi·∫£m chi ph√≠ t·ª´ 60-70% so v·ªõi On-Demand instances.\nSpot Instances l√† g√¨? Spot Instances l√† c√°c EC2 instances kh√¥ng s·ª≠ d·ª•ng c·ªßa AWS, ƒë∆∞·ª£c b√°n v·ªõi gi√° th·∫•p h∆°n nhi·ªÅu so v·ªõi On-Demand. Tuy nhi√™n, AWS c√≥ th·ªÉ thu h·ªìi ch√∫ng b·∫•t c·ª© l√∫c n√†o khi c√≥ nhu c·∫ßu.\n∆Øu ƒëi·ªÉm:\nGi√° r·∫ª h∆°n 50-90% so v·ªõi On-Demand Ph√π h·ª£p cho workloads c√≥ th·ªÉ ch·ªãu ƒë∆∞·ª£c interruption T√≠ch h·ª£p t·ªët v·ªõi EMR Nh∆∞·ª£c ƒëi·ªÉm:\nC√≥ th·ªÉ b·ªã terminate b·∫•t c·ª© l√∫c n√†o Kh√¥ng ph√π h·ª£p cho workloads critical C·∫ßn thi·∫øt k·∫ø application ƒë·ªÉ handle interruption Chi·∫øn l∆∞·ª£c s·ª≠ d·ª•ng Spot v·ªõi EMR 1. Mixed Instance Types S·ª≠ d·ª•ng nhi·ªÅu lo·∫°i instance kh√°c nhau ƒë·ªÉ tƒÉng availability:\nm5.large, m4.large, c5.large N·∫øu m·ªôt lo·∫°i b·ªã terminate, c√°c lo·∫°i kh√°c v·∫´n ch·∫°y 2. Ph√¢n chia vai tr√≤ Master node: Lu√¥n d√πng On-Demand (quan tr·ªçng nh·∫•t) Core nodes: M·ªôt ph·∫ßn On-Demand, m·ªôt ph·∫ßn Spot Task nodes: 100% Spot (c√≥ th·ªÉ terminate m√† kh√¥ng m·∫•t data) 3. Bidding Strategy ƒê·∫∑t bid price cao h∆°n current spot price 10-20% Theo d√µi spot price history ƒë·ªÉ ƒë·∫∑t gi√° h·ª£p l√Ω Th·ª±c h√†nh: T·∫°o EMR Cluster v·ªõi Spot B∆∞·ªõc 1: Chu·∫©n b·ªã Tr∆∞·ªõc khi t·∫°o cluster, b·∫°n c·∫ßn:\nƒêƒÉng nh·∫≠p AWS Console Ch·ªçn region (khuy·∫øn ngh·ªã: us-east-1) T·∫°o EC2 Key Pair n·∫øu ch∆∞a c√≥ Ki·ªÉm tra IAM roles: EMR_DefaultRole v√† EMR_EC2_DefaultRole B∆∞·ªõc 2: T·∫°o Cluster qua Console M·ªü AWS Console ‚Üí EMR Click \u0026ldquo;Create cluster\u0026rdquo; Ch·ªçn \u0026ldquo;Go to advanced options\u0026rdquo; C·∫•u h√¨nh Software:\nRelease: emr-6.15.0 Applications: Spark, Hadoop C·∫•u h√¨nh Hardware:\nMaster: 1 x m5.xlarge (On-Demand) Core: 1 x m5.large (On-Demand) + 2 x m5.large (Spot, bid $0.05) Task: 4 x m5.large (Spot, bid $0.05) C·∫•u h√¨nh General:\nCluster name: \u0026ldquo;Workshop-Spot-Cluster\u0026rdquo; Logging: Enable (ch·ªçn S3 bucket) Termination protection: Disable B∆∞·ªõc 3: Theo d√µi Cluster Sau khi t·∫°o cluster:\nTheo d√µi tr·∫°ng th√°i trong EMR Console Ki·ªÉm tra Hardware tab ƒë·ªÉ xem instances Xem Spot price history trong EC2 Console B∆∞·ªõc 4: So s√°nh Chi ph√≠ C·∫•u h√¨nh On-Demand:\nMaster: 1 x m5.xlarge = $0.192/gi·ªù Workers: 6 x m5.large = $0.576/gi·ªù T·ªïng: $0.768/gi·ªù C·∫•u h√¨nh Spot:\nMaster: 1 x m5.xlarge = $0.192/gi·ªù Core On-Demand: 1 x m5.large = $0.096/gi·ªù Spot instances: 6 x m5.large = $0.180/gi·ªù (gi·∫£ s·ª≠ spot price $0.03) T·ªïng: $0.468/gi·ªù Ti·∫øt ki·ªám: 39% ($0.30/gi·ªù)\nX·ª≠ l√Ω Spot Interruption Monitoring Interruptions EMR t·ª± ƒë·ªông x·ª≠ l√Ω spot interruptions:\nTask nodes b·ªã terminate: Jobs ƒë∆∞·ª£c redistribute Core nodes b·ªã terminate: Data ƒë∆∞·ª£c replicate Cluster ti·∫øp t·ª•c ch·∫°y v·ªõi instances c√≤n l·∫°i Best Practices Checkpoint th∆∞·ªùng xuy√™n: L∆∞u intermediate results S·ª≠ d·ª•ng S3: Store data ngo√†i cluster Mixed instance types: Gi·∫£m risk t·∫•t c·∫£ b·ªã terminate c√πng l√∫c Monitor spot prices: Adjust bid prices khi c·∫ßn Lab Exercise: Test Spot Interruption T·∫°o Test Job SSH v√†o master node T·∫°o file test job: # simple-job.py from pyspark.sql import SparkSession import time spark = SparkSession.builder.appName(\u0026#34;SpotTest\u0026#34;).getOrCreate() # T·∫°o large dataset df = spark.range(0, 10000000).toDF(\u0026#34;id\u0026#34;) df = df.repartition(100) # Ch·∫°y job l√¢u ƒë·ªÉ test interruption for i in range(10): result = df.count() print(f\u0026#34;Iteration {i}: Count = {result}\u0026#34;) time.sleep(60) # Ch·ªù 1 ph√∫t spark.stop() Submit job: spark-submit simple-job.py Simulate Interruption Trong EC2 Console, terminate m·ªôt spot instance Quan s√°t job v·∫´n ti·∫øp t·ª•c ch·∫°y Ki·ªÉm tra EMR Console xem cluster status K·∫øt qu·∫£ Ph·∫ßn 1 Sau khi ho√†n th√†nh ph·∫ßn n√†y, b·∫°n ƒë√£: ‚úÖ Hi·ªÉu c√°ch Spot Instances ho·∫°t ƒë·ªông v·ªõi EMR ‚úÖ T·∫°o ƒë∆∞·ª£c cluster v·ªõi mixed instance types ‚úÖ Ti·∫øt ki·ªám 39% chi ph√≠ so v·ªõi On-Demand ‚úÖ Test ƒë∆∞·ª£c spot interruption handling Chi ph√≠ th·ª±c t·∫ø: Cluster ch·∫°y 1 gi·ªù = ~$0.47 (thay v√¨ $0.77) Ch√∫c m·ª´ng! B·∫°n ƒë√£ t·∫°o th√†nh c√¥ng EMR cluster v·ªõi Spot Instances v√† ti·∫øt ki·ªám ƒë∆∞·ª£c chi ph√≠ ƒë√°ng k·ªÉ. Ti·∫øp theo, ch√∫ng ta s·∫Ω h·ªçc Auto Scaling ƒë·ªÉ t·ªëi ∆∞u h√≥a th√™m.\rL∆∞u √Ω: ƒê·ª´ng terminate cluster ngay! Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng cluster n√†y cho Ph·∫ßn 2: Auto Scaling.\rC√¢u h·ªèi th∆∞·ªùng g·∫∑p Q: Spot instances c√≥ ƒë√°ng tin c·∫≠y kh√¥ng? A: V·ªõi EMR, spot instances r·∫•t ƒë√°ng tin c·∫≠y v√¨ EMR t·ª± ƒë·ªông handle interruptions. Ch·ªâ c·∫ßn thi·∫øt k·∫ø job properly. **Q:Khi n√†o n√™n s·ª≠ d·ª•ng Spot instances?** A: Spot instances ph√π h·ª£p cho batch processing, data analysis, machine learning training - c√°c workloads c√≥ th·ªÉ restart ƒë∆∞·ª£c. **Q: L√†m sao bi·∫øt bid price ph√π h·ª£p?** A: Ki·ªÉm tra Spot Price History trong EC2 Console, ƒë·∫∑t bid cao h∆°n average price 10-20%. **Q: N·∫øu t·∫•t c·∫£ spot instances b·ªã terminate th√¨ sao?** A: EMR s·∫Ω t·ª± ƒë·ªông launch instances m·ªõi. Job c√≥ th·ªÉ restart t·ª´ checkpoint g·∫ßn nh·∫•t. --- **Ti·∫øp theo:** [Ph·∫ßn 2: Auto Scaling](/02-auto-scaling) ƒë·ªÉ h·ªçc c√°ch t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh cluster size theo workload. "
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/2-auto-scaling/",
	"title": "Ph·∫ßn 2: Auto Scaling",
	"tags": [],
	"description": "",
	"content": "Ph·∫ßn 2: EMR Auto Scaling - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh theo workload Trong ph·∫ßn n√†y, b·∫°n s·∫Ω h·ªçc c√°ch thi·∫øt l·∫≠p EMR Auto Scaling ƒë·ªÉ cluster t·ª± ƒë·ªông tƒÉng gi·∫£m s·ªë l∆∞·ª£ng instances theo workload, gi√∫p t·ªëi ∆∞u h√≥a c·∫£ chi ph√≠ v√† performance.\nEMR Auto Scaling l√† g√¨? EMR Auto Scaling t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng instances trong cluster d·ª±a tr√™n:\nYARN metrics: Memory v√† CPU utilization Custom metrics: CloudWatch metrics t√πy ch·ªânh Time-based scaling: Theo l·ªãch tr√¨nh ƒë·ªãnh s·∫µn L·ª£i √≠ch:\nTi·∫øt ki·ªám chi ph√≠ khi workload th·∫•p TƒÉng performance khi workload cao Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng T√≠ch h·ª£p t·ªët v·ªõi Spot Instances C√°c lo·∫°i Auto Scaling 1. EMR Managed Scaling (Khuy·∫øn ngh·ªã) AWS qu·∫£n l√Ω ho√†n to√†n D·ª±a tr√™n YARN container pending ƒê∆°n gi·∫£n, hi·ªáu qu·∫£ H·ªó tr·ª£ c·∫£ On-Demand v√† Spot 2. Custom Auto Scaling T·ª± ƒë·ªãnh nghƒ©a scaling policies D·ª±a tr√™n CloudWatch metrics Linh ho·∫°t h∆°n nh∆∞ng ph·ª©c t·∫°p Ph√π h·ª£p cho use cases ƒë·∫∑c bi·ªát Th·ª±c h√†nh: Thi·∫øt l·∫≠p Managed Scaling B∆∞·ªõc 1: Enable Managed Scaling S·ª≠ d·ª•ng cluster t·ª´ Ph·∫ßn 1, ch√∫ng ta s·∫Ω enable auto scaling:\nM·ªü EMR Console Ch·ªçn cluster \u0026ldquo;Workshop-Spot-Cluster\u0026rdquo; V√†o tab \u0026ldquo;Configuration\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; ·ªü ph·∫ßn \u0026ldquo;Scaling\u0026rdquo; C·∫•u h√¨nh Managed Scaling:\nMinimum capacity: 2 instances Maximum capacity: 10 instances Maximum On-Demand capacity: 4 instances B∆∞·ªõc 2: C·∫•u h√¨nh Advanced Settings Scale-out settings:\nScale out cooldown: 300 seconds Maximum scale-out increment: 100% Scale-in settings:\nScale in cooldown: 300 seconds Maximum scale-in increment: 50% B∆∞·ªõc 3: Ki·ªÉm tra Configuration Sau khi enable, ki·ªÉm tra:\nScaling status: \u0026ldquo;Enabled\u0026rdquo; Current capacity: 7 instances (t·ª´ Ph·∫ßn 1) Target capacity: S·∫Ω thay ƒë·ªïi theo workload Test Auto Scaling T·∫°o Workload ƒë·ªÉ Test Scaling SSH v√†o Master Node: ssh -i your-key.pem hadoop@master-public-ip\nT·∫°o Test Script: ``python\nscaling-test.py from pyspark.sql import SparkSession import time\nspark = SparkSession.builder .appName(\u0026ldquo;AutoScalingTest\u0026rdquo;) .config(\u0026ldquo;spark.sql.adaptive.enabled\u0026rdquo;, \u0026ldquo;false\u0026rdquo;) .config(\u0026ldquo;spark.sql.adaptive.coalescePartitions.enabled\u0026rdquo;, \u0026ldquo;false\u0026rdquo;) .getOrCreate()\nprint(\u0026quot;=== Starting Auto Scaling Test ===\u0026quot;)\nT·∫°o large dataset ƒë·ªÉ trigger scaling print(\u0026ldquo;Creating large dataset\u0026hellip;\u0026rdquo;) df = spark.range(0, 100000000).toDF(\u0026ldquo;id\u0026rdquo;) df = df.repartition(200) # Nhi·ªÅu partitions ƒë·ªÉ c·∫ßn nhi·ªÅu resources\nCache ƒë·ªÉ consume memory df.cache()\nCh·∫°y multiple operations ƒë·ªÉ maintain load for i in range(5): print(f\u0026quot;Running operation {i+1}/5\u0026hellip;\u0026quot;)\n# Heavy computation\rresult = df.filter(df.id % 2 == 0).count()\rprint(f\u0026quot;Even numbers count: {result}\u0026quot;)\r# Gi·ªØ load trong 5 ph√∫t ƒë·ªÉ observe scaling\rtime.sleep(300)\rprint(\u0026quot;=== Test completed ===\u0026quot;) spark.stop() ``\nSubmit Job: spark-submit \\\r--executor-memory 2g \\\r--num-executors 15 \\\r--executor-cores 2 \\\rscaling-test.py Theo d√µi Scaling Process Trong EMR Console:\nV√†o tab \u0026ldquo;Hardware\u0026rdquo; ƒë·ªÉ xem instances Refresh m·ªói 2-3 ph√∫t Quan s√°t s·ªë l∆∞·ª£ng instances tƒÉng l√™n Trong CloudWatch:\nM·ªü CloudWatch Console V√†o \u0026ldquo;Metrics\u0026rdquo; ‚Üí \u0026ldquo;AWS/ElasticMapReduce\u0026rdquo; Ch·ªçn metrics: YARNMemoryAvailablePercentage ContainerPending AppsRunning Expected Behavior:\nPh√∫t 0-2: Job b·∫Øt ƒë·∫ßu, YARN memory gi·∫£m Ph√∫t 2-5: ContainerPending tƒÉng, scaling triggered Ph√∫t 5-8: Instances m·ªõi ƒë∆∞·ª£c add (2-3 instances) Ph√∫t 8-25: Job ch·∫°y v·ªõi capacity m·ªõi Ph√∫t 25-30: Job k·∫øt th√∫c, scaling down b·∫Øt ƒë·∫ßu Monitoring v√† Troubleshooting Key Metrics ƒë·ªÉ Monitor YARN Metrics:\nYARNMemoryAvailablePercentage: \u0026lt; 15% trigger scale out ContainerPending: \u0026gt; 0 trong 5 ph√∫t trigger scale out AppsRunning: S·ªë applications ƒëang ch·∫°y EMR Metrics:\nRunningMapTasks: Map tasks ƒëang ch·∫°y RunningReduceTasks: Reduce tasks ƒëang ch·∫°y TotalLoad: T·ªïng load c·ªßa cluster Common Issues v√† Solutions Issue 1: Scaling kh√¥ng ho·∫°t ƒë·ªông\nKi·ªÉm tra IAM permissions Verify scaling limits (min/max capacity) Check cooldown periods Issue 2: Scale out qu√° ch·∫≠m\nGi·∫£m scale-out cooldown TƒÉng maximum scale-out increment S·ª≠ d·ª•ng multiple instance types Issue 3: Scale in qu√° nhanh\nTƒÉng scale-in cooldown Gi·∫£m maximum scale-in increment Adjust YARN memory thresholds Advanced Scaling Strategies 1. Mixed Instance Types cho Scaling C·∫•u h√¨nh multiple instance types ƒë·ªÉ tƒÉng availability:\nInstance Fleet Configuration:\nPrimary: m5.large (Spot) Secondary: m4.large (Spot) Fallback: c5.large (Spot) Emergency: m5.large (On-Demand) 2. Time-based Scaling Cho workloads c√≥ pattern c·ªë ƒë·ªãnh:\nScale out tr∆∞·ªõc peak hours Scale in sau off-peak hours S·ª≠ d·ª•ng CloudWatch Events + Lambda 3. Predictive Scaling D·ª±a tr√™n historical data:\nAnalyze past workload patterns Pre-scale cho expected load Combine v·ªõi reactive scaling Cost Optimization v·ªõi Auto Scaling Before Auto Scaling (Static Cluster) Peak capacity: 10 instances √ó 8 hours = 80 instance-hours Cost: 80 √ó $0.096 = $7.68/day After Auto Scaling (Dynamic Cluster) Average capacity: 4 instances √ó 8 hours = 32 instance-hours Peak capacity: 8 instances √ó 2 hours = 16 instance-hours Total: 32 + 16 = 48 instance-hours Cost: 48 √ó $0.096 = $4.61/day Ti·∫øt ki·ªám: $3.07/day (40% cost reduction)\nLab Exercise: Custom Scaling Policy T·∫°o Custom CloudWatch Alarm T·∫°o Scale-Out Alarm: aws cloudwatch put-metric-alarm \\\r--alarm-name \u0026quot;EMR-ScaleOut-HighMemory\u0026quot; \\\r--alarm-description \u0026quot;Scale out when memory usage \u0026gt; 80%\u0026quot; \\\r--metric-name YARNMemoryAvailablePercentage \\\r--namespace AWS/ElasticMapReduce \\\r--statistic Average \\\r--period 300 \\\r--threshold 20 \\\r--comparison-operator LessThanThreshold \\\r--evaluation-periods 2 \\\r--dimensions Name=JobFlowId,Value=j-xxxxx\nT·∫°o Scale-In Alarm: aws cloudwatch put-metric-alarm \\\r--alarm-name \u0026quot;EMR-ScaleIn-LowMemory\u0026quot; \\\r--alarm-description \u0026quot;Scale in when memory usage \u0026lt; 30%\u0026quot; \\\r--metric-name YARNMemoryAvailablePercentage \\\r--namespace AWS/ElasticMapReduce \\\r--statistic Average \\\r--period 600 \\\r--threshold 70 \\\r--comparison-operator GreaterThanThreshold \\\r--evaluation-periods 3 \\\r--dimensions Name=JobFlowId,Value=j-xxxxx\nTest Custom Scaling T·∫°o Light Workload: ``python light-workload.py from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\u0026ldquo;LightWorkload\u0026rdquo;).getOrCreate()\nSmall dataset df = spark.range(0, 1000000).toDF(\u0026ldquo;id\u0026rdquo;) result = df.count() print(f\u0026quot;Light workload result: {result}\u0026quot;)\nspark.stop() ``\nSubmit v√† Monitor: spark-submit light-workload.py\nQuan s√°t Scale-In:\nMemory usage gi·∫£m xu·ªëng \u0026lt; 30% Sau 10 ph√∫t, cluster scale in Instances gi·∫£m t·ª´ 8 xu·ªëng 4 Production Best Practices 1. Scaling Configuration Recommended Settings:\nMin capacity: 20% c·ªßa peak capacity Max capacity: 150% c·ªßa expected peak Scale-out cooldown: 300 seconds Scale-in cooldown: 600 seconds 2. Instance Mix Strategy Optimal Mix:\n30% On-Demand (stability) 70% Spot (cost savings) Multiple instance families Diversified AZs 3. Application Design Scaling-Friendly Applications:\nStateless processing Checkpointing enabled Graceful handling c·ªßa node loss Partition data appropriately 4. Monitoring Setup Essential Metrics:\nCluster utilization Scaling events Job completion times Cost per job K·∫øt qu·∫£ Ph·∫ßn 2 Sau khi ho√†n th√†nh ph·∫ßn n√†y, b·∫°n ƒë√£:\n‚úÖ Thi·∫øt l·∫≠p EMR Managed Scaling th√†nh c√¥ng ‚úÖ Test scaling v·ªõi real workload ‚úÖ Hi·ªÉu c√°ch monitor scaling metrics ‚úÖ T·ªëi ∆∞u h√≥a th√™m 40% chi ph√≠ v·ªõi dynamic scaling T·ªïng ti·∫øt ki·ªám ƒë·∫øn gi·ªù:\nSpot Instances: 39% (t·ª´ Ph·∫ßn 1) Auto Scaling: 40% (t·ª´ Ph·∫ßn 2) Combined savings: ~65% so v·ªõi static On-Demand cluster Xu·∫•t s·∫Øc! Cluster c·ªßa b·∫°n gi·ªù ƒë√£ t·ª± ƒë·ªông scale theo workload v√† ti·∫øt ki·ªám t·ªëi ƒëa chi ph√≠. Ti·∫øp theo, ch√∫ng ta s·∫Ω thi·∫øt l·∫≠p monitoring ƒë·ªÉ theo d√µi m·ªçi th·ª©.\nTroubleshooting Common Issues Issue: Scaling Events kh√¥ng xu·∫•t hi·ªán Nguy√™n nh√¢n c√≥ th·ªÉ:\nIAM role thi·∫øu permissions Cooldown period ch∆∞a h·∫øt Metrics ch∆∞a ƒë·∫°t threshold Gi·∫£i ph√°p:\nKi·ªÉm tra CloudTrail logs Verify EMR service role permissions Adjust threshold values Issue: Scale-in qu√° aggressive Tri·ªáu ch·ª©ng:\nInstances b·ªã terminate khi job v·∫´n ch·∫°y Performance degradation Gi·∫£i ph√°p:\nTƒÉng scale-in cooldown l√™n 900s Gi·∫£m maximum scale-in increment xu·ªëng 25% Set higher memory threshold (80% thay v√¨ 70%) Issue: Spot instances kh√¥ng ƒë∆∞·ª£c add khi scaling Nguy√™n nh√¢n:\nSpot capacity kh√¥ng available Bid price qu√° th·∫•p Instance type constraints Gi·∫£i ph√°p:\nTh√™m multiple instance types TƒÉng bid price Enable multiple AZs Performance Tuning Tips 1. Optimize Spark Configuration # Spark configs cho auto-scaling environment\rspark.dynamicAllocation.enabled=true\rspark.dynamicAllocation.minExecutors=2\rspark.dynamicAllocation.maxExecutors=50\rspark.dynamicAllocation.initialExecutors=5\n2. YARN Configuration ``xml\n3. EMR Steps Optimization S·ª≠ d·ª•ng cluster mode thay v√¨ client mode Enable speculation cho fault tolerance Configure appropriate parallelism Real-world Example: E-commerce Analytics Scenario C√¥ng ty e-commerce c·∫ßn process log data h√†ng ng√†y:\nMorning (6-10 AM): Light processing (2-3 instances) Afternoon (2-6 PM): Heavy analytics (8-12 instances) Night (10 PM-2 AM): Batch reports (4-6 instances) Auto Scaling Configuration # Managed scaling policy\r{\r\u0026quot;ComputeLimits\u0026quot;: {\r\u0026quot;UnitType\u0026quot;: \u0026quot;Instances\u0026quot;,\r\u0026quot;MinimumCapacityUnits\u0026quot;: 2,\r\u0026quot;MaximumCapacityUnits\u0026quot;: 15,\r\u0026quot;MaximumOnDemandCapacityUnits\u0026quot;: 5,\r\u0026quot;MaximumCoreCapacityUnits\u0026quot;: 8\r}\r}\nCost Comparison Before Auto Scaling:\nStatic 12 instances √ó 24 hours = 288 instance-hours Cost: 288 √ó $0.096 = $27.65/day After Auto Scaling:\nAverage 5 instances √ó 24 hours = 120 instance-hours Cost: 120 √ó $0.096 = $11.52/day Savings: $16.13/day (58%) Advanced Monitoring Setup Custom Metrics Dashboard T·∫°o CloudWatch dashboard v·ªõi:\nCluster capacity over time Cost per hour tracking Job completion rates Scaling events timeline Automated Alerts Setup alerts cho:\nScaling failures High cost thresholds Performance degradation Spot interruption rates Cost Tracking Script ``python\ncost-tracker.py import boto3 from datetime import datetime, timedelta\ndef track_emr_costs(cluster_id): emr = boto3.client(\u0026rsquo;emr\u0026rsquo;) ce = boto3.client(\u0026lsquo;ce\u0026rsquo;)\n# Get cluster info\rcluster = emr.describe_cluster(ClusterId=cluster_id)\rstart_time = cluster['Cluster']['Status']['Timeline']['CreationDateTime']\r# Calculate cost\rend_time = datetime.now()\rresponse = ce.get_cost_and_usage(\rTimePeriod={\r'Start': start_time.strftime('%Y-%m-%d'),\r'End': end_time.strftime('%Y-%m-%d')\r},\rGranularity='DAILY',\rMetrics=['BlendedCost'],\rGroupBy=[\r{'Type': 'DIMENSION', 'Key': 'SERVICE'}\r]\r)\rprint(f\u0026quot;Cluster {cluster_id} cost tracking:\u0026quot;)\rfor result in response['ResultsByTime']:\rfor group in result['Groups']:\rif 'ElasticMapReduce' in group['Keys'][0]:\rcost = group['Metrics']['BlendedCost']['Amount']\rprint(f\u0026quot;Date: {result['TimePeriod']['Start']}, Cost: ${cost}\u0026quot;)\rUsage track_emr_costs(\u0026lsquo;j-xxxxx\u0026rsquo;) ``\nScaling Patterns Analysis Pattern 1: Batch Processing Characteristics:\nPredictable workload times High resource usage during processing Idle periods between jobs Optimal Strategy:\nAggressive scale-out (200% increment) Conservative scale-in (25% increment) Longer cooldown periods (600s) Pattern 2: Interactive Analytics Characteristics:\nUnpredictable query patterns Variable resource requirements Need for quick response times Optimal Strategy:\nModerate scale-out (100% increment) Quick scale-in (50% increment) Shorter cooldown periods (300s) Pattern 3: Streaming Workloads Characteristics:\nContinuous data processing Steady resource usage Occasional spikes Optimal Strategy:\nConservative scaling (50% increment) Maintain minimum baseline Focus on stability over cost Integration v·ªõi Other AWS Services 1. Lambda Triggers T·ª± ƒë·ªông start/stop clusters: ``python\nlambda-emr-scheduler.py import boto3 import json\ndef lambda_handler(event, context): emr = boto3.client(\u0026rsquo;emr\u0026rsquo;)\nif event['action'] == 'start':\r# Start cluster with auto-scaling\rresponse = emr.run_job_flow(\rName='Scheduled-Cluster',\rReleaseLabel='emr-6.15.0',\rInstances={\r'MasterInstanceType': 'm5.xlarge',\r'SlaveInstanceType': 'm5.large',\r'InstanceCount': 3,\r'Ec2KeyName': 'your-key'\r},\rApplications=[{'Name': 'Spark'}],\rServiceRole='EMR_DefaultRole',\rJobFlowRole='EMR_EC2_DefaultRole'\r)\rcluster_id = response['JobFlowId']\r# Enable managed scaling\remr.put_managed_scaling_policy(\rClusterId=cluster_id,\rManagedScalingPolicy={\r'ComputeLimits': {\r'UnitType': 'Instances',\r'MinimumCapacityUnits': 2,\r'MaximumCapacityUnits': 10\r}\r}\r)\rreturn {'statusCode': 200, 'body': f'Started cluster: {cluster_id}'}\relif event['action'] == 'stop':\r# Terminate cluster\remr.terminate_job_flows(JobFlowIds=[event['cluster_id']])\rreturn {'statusCode': 200, 'body': 'Cluster terminated'}\r``\n2. Step Functions Orchestration Workflow cho complex data pipelines: json\r{\r\u0026quot;Comment\u0026quot;: \u0026quot;EMR Auto-scaling Pipeline\u0026quot;,\r\u0026quot;StartAt\u0026quot;: \u0026quot;CreateCluster\u0026quot;,\r\u0026quot;States\u0026quot;: {\r\u0026quot;CreateCluster\u0026quot;: {\r\u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::emr:createCluster.sync\u0026quot;,\r\u0026quot;Parameters\u0026quot;: {\r\u0026quot;Name\u0026quot;: \u0026quot;Pipeline-Cluster\u0026quot;,\r\u0026quot;ReleaseLabel\u0026quot;: \u0026quot;emr-6.15.0\u0026quot;\r},\r\u0026quot;Next\u0026quot;: \u0026quot;EnableScaling\u0026quot;\r},\r\u0026quot;EnableScaling\u0026quot;: {\r\u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:emr:putManagedScalingPolicy\u0026quot;,\r\u0026quot;Parameters\u0026quot;: {\r\u0026quot;ClusterId.$\u0026quot;: \u0026quot;$.ClusterId\u0026quot;,\r\u0026quot;ManagedScalingPolicy\u0026quot;: {\r\u0026quot;ComputeLimits\u0026quot;: {\r\u0026quot;UnitType\u0026quot;: \u0026quot;Instances\u0026quot;,\r\u0026quot;MinimumCapacityUnits\u0026quot;: 2,\r\u0026quot;MaximumCapacityUnits\u0026quot;: 20\r}\r}\r},\r\u0026quot;Next\u0026quot;: \u0026quot;ProcessData\u0026quot;\r},\r\u0026quot;ProcessData\u0026quot;: {\r\u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::emr:addStep.sync\u0026quot;,\r\u0026quot;End\u0026quot;: true\r}\r}\r}\r3. EventBridge Rules T·ª± ƒë·ªông respond to scaling events: json\r{\r\u0026quot;Rules\u0026quot;: [\r{\r\u0026quot;Name\u0026quot;: \u0026quot;EMR-ScaleOut-Alert\u0026quot;,\r\u0026quot;EventPattern\u0026quot;: {\r\u0026quot;source\u0026quot;: [\u0026quot;aws.emr\u0026quot;],\r\u0026quot;detail-type\u0026quot;: [\u0026quot;EMR Instance Group State Change\u0026quot;],\r\u0026quot;detail\u0026quot;: {\r\u0026quot;state\u0026quot;: [\u0026quot;RUNNING\u0026quot;],\r\u0026quot;requestedInstanceCount\u0026quot;: {\r\u0026quot;numeric\u0026quot;: [\u0026quot;\u0026gt;\u0026quot;, 5]\r}\r}\r},\r\u0026quot;Targets\u0026quot;: [\r{\r\u0026quot;Id\u0026quot;: \u0026quot;1\u0026quot;,\r\u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sns:us-east-1:123456789012:emr-alerts\u0026quot;\r}\r]\r}\r]\r}\rFinal Lab: End-to-End Scenario Scenario Setup T·∫°o m·ªôt complete data processing pipeline:\nData Ingestion: S3 ‚Üí EMR Processing: Spark jobs v·ªõi auto-scaling Output: Results ‚Üí S3 Monitoring: CloudWatch dashboard Cleanup: Automatic termination Implementation Steps Upload Sample Data: aws s3 cp sample-data.csv s3://your-bucket/input/\nCreate Processing Script: ``python\nend-to-end-pipeline.py from pyspark.sql import SparkSession from pyspark.sql.functions import *\nspark = SparkSession.builder.appName(\u0026ldquo;E2EPipeline\u0026rdquo;).getOrCreate()\nRead data from S3 df = spark.read.csv(\u0026ldquo;s3://your-bucket/input/sample-data.csv\u0026rdquo;, header=True)\nHeavy processing to trigger scaling df_processed = df.groupBy(\u0026ldquo;category\u0026rdquo;).agg( count(\u0026quot;*\u0026quot;).alias(\u0026ldquo;count\u0026rdquo;), avg(\u0026ldquo;value\u0026rdquo;).alias(\u0026ldquo;avg_value\u0026rdquo;), max(\u0026ldquo;value\u0026rdquo;).alias(\u0026ldquo;max_value\u0026rdquo;) ).repartition(50) # Force many partitions\nCache to consume memory df_processed.cache()\nMultiple operations result1 = df_processed.filter(col(\u0026ldquo;count\u0026rdquo;) \u0026gt; 100).count() result2 = df_processed.orderBy(desc(\u0026ldquo;avg_value\u0026rdquo;)).collect()\nWrite results df_processed.write.mode(\u0026ldquo;overwrite\u0026rdquo;).csv(\u0026ldquo;s3://your-bucket/output/\u0026rdquo;)\nprint(f\u0026quot;Pipeline completed. Processed {result1} categories\u0026quot;) spark.stop() ``\nSubmit Pipeline: aws emr add-steps --cluster-id j-xxxxx \\\r--steps '[{\r\u0026quot;Name\u0026quot;: \u0026quot;E2E-Pipeline\u0026quot;,\r\u0026quot;ActionOnFailure\u0026quot;: \u0026quot;TERMINATE_CLUSTER\u0026quot;,\r\u0026quot;HadoopJarStep\u0026quot;: {\r\u0026quot;Jar\u0026quot;: \u0026quot;command-runner.jar\u0026quot;,\r\u0026quot;Args\u0026quot;: [\r\u0026quot;spark-submit\u0026quot;,\r\u0026quot;--executor-memory\u0026quot;, \u0026quot;2g\u0026quot;,\r\u0026quot;--num-executors\u0026quot;, \u0026quot;20\u0026quot;,\r\u0026quot;s3://your-bucket/scripts/end-to-end-pipeline.py\u0026quot;\r]\r}\r}]'\nMonitor Complete Workflow:\nCluster starts v·ªõi 3 instances Job b·∫Øt ƒë·∫ßu, memory usage tƒÉng Auto-scaling kicks in, th√™m 4-6 instances Processing completes Scale-in begins, gi·∫£m v·ªÅ 3 instances Job finishes, cluster c√≥ th·ªÉ terminate Expected Timeline 0-5 min: Job startup, initial processing 5-10 min: Heavy load, scaling out to 8-10 instances 10-25 min: Processing v·ªõi full capacity 25-30 min: Job completion, scaling in 30-35 min: Final cleanup, cluster ready for next job Performance Metrics Analysis Key Performance Indicators (KPIs) Cost Efficiency:\nCost per GB processed Cost per job completion Utilization percentage Performance:\nJob completion time Throughput (GB/hour) Resource efficiency Reliability:\nSuccess rate Spot interruption impact Recovery time Benchmarking Results Static Cluster (Baseline):\n10 instances √ó 2 hours = 20 instance-hours Cost: $1.92 Processing time: 45 minutes Utilization: 60% Auto-Scaling Cluster:\nAverage 6 instances √ó 2 hours = 12 instance-hours Cost: $1.15 Processing time: 50 minutes Utilization: 85% Improvement:\n40% cost reduction 25% better utilization Only 11% longer processing time Graduation Exercise Challenge: Optimize Real Workload B·∫°n ƒë∆∞·ª£c cung c·∫•p m·ªôt dataset 10GB v√† y√™u c·∫ßu:\nProcess data v·ªõi budget t·ªëi ƒëa $2 Complete trong 60 ph√∫t Achieve 80%+ cluster utilization Handle √≠t nh·∫•t 1 spot interruption Solution Approach Cluster Configuration: json\r{\r\u0026quot;InstanceGroups\u0026quot;: [\r{\r\u0026quot;Name\u0026quot;: \u0026quot;Master\u0026quot;,\r\u0026quot;InstanceRole\u0026quot;: \u0026quot;MASTER\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;m5.large\u0026quot;,\r\u0026quot;InstanceCount\u0026quot;: 1,\r\u0026quot;Market\u0026quot;: \u0026quot;ON_DEMAND\u0026quot;\r},\r{\r\u0026quot;Name\u0026quot;: \u0026quot;Core\u0026quot;,\r\u0026quot;InstanceRole\u0026quot;: \u0026quot;CORE\u0026quot;,\r\u0026quot;InstanceType\u0026quot;: \u0026quot;m5.large\u0026quot;, \u0026quot;InstanceCount\u0026quot;: 1,\r\u0026quot;Market\u0026quot;: \u0026quot;ON_DEMAND\u0026quot;\r},\r{\r\u0026quot;Name\u0026quot;: \u0026quot;Task\u0026quot;,\r\u0026quot;InstanceRole\u0026quot;: \u0026quot;TASK\u0026quot;,\r\u0026quot;InstanceType\u0026quot;: \u0026quot;m5.large\u0026quot;,\r\u0026quot;InstanceCount\u0026quot;: 0,\r\u0026quot;Market\u0026quot;: \u0026quot;SPOT\u0026quot;,\r\u0026quot;BidPrice\u0026quot;: \u0026quot;0.04\u0026quot;\r}\r]\r}\rScaling Policy: json\r{\r\u0026quot;ComputeLimits\u0026quot;: {\r\u0026quot;UnitType\u0026quot;: \u0026quot;Instances\u0026quot;,\r\u0026quot;MinimumCapacityUnits\u0026quot;: 2,\r\u0026quot;MaximumCapacityUnits\u0026quot;: 12,\r\u0026quot;MaximumOnDemandCapacityUnits\u0026quot;: 2,\r\u0026quot;MaximumCoreCapacityUnits\u0026quot;: 2\r}\r}\rOptimized Spark Job: ``python\noptimized-processing.py from pyspark.sql import SparkSession from pyspark.sql.functions import *\nspark = SparkSession.builder .appName(\u0026ldquo;OptimizedProcessing\u0026rdquo;) .config(\u0026ldquo;spark.sql.adaptive.enabled\u0026rdquo;, \u0026ldquo;true\u0026rdquo;) .config(\u0026ldquo;spark.sql.adaptive.coalescePartitions.enabled\u0026rdquo;, \u0026ldquo;true\u0026rdquo;) .config(\u0026ldquo;spark.serializer\u0026rdquo;, \u0026ldquo;org.apache.spark.serializer.KryoSerializer\u0026rdquo;) .getOrCreate()\nEfficient data reading df = spark.read.parquet(\u0026ldquo;s3://your-bucket/data/\u0026rdquo;) .repartition(100) # Optimal partitioning\nCheckpoint ƒë·ªÉ handle spot interruptions spark.sparkContext.setCheckpointDir(\u0026ldquo;s3://your-bucket/checkpoints/\u0026rdquo;) df.checkpoint()\nProcessing v·ªõi caching strategy df_processed = df.groupBy(\u0026ldquo;category\u0026rdquo;, \u0026ldquo;date\u0026rdquo;) .agg( sum(\u0026ldquo;amount\u0026rdquo;).alias(\u0026ldquo;total_amount\u0026rdquo;), count(\u0026quot;*\u0026quot;).alias(\u0026ldquo;transaction_count\u0026rdquo;), avg(\u0026ldquo;amount\u0026rdquo;).alias(\u0026ldquo;avg_amount\u0026rdquo;) ) .cache()\nMultiple outputs ƒë·ªÉ maximize resource usage df_processed.write.mode(\u0026ldquo;overwrite\u0026rdquo;) .partitionBy(\u0026ldquo;date\u0026rdquo;) .parquet(\u0026ldquo;s3://your-bucket/output/summary/\u0026rdquo;)\ndf_processed.filter(col(\u0026ldquo;total_amount\u0026rdquo;) \u0026gt; 1000) .write.mode(\u0026ldquo;overwrite\u0026rdquo;) .json(\u0026ldquo;s3://your-bucket/output/high-value/\u0026rdquo;)\nspark.stop() ``\nSuccess Criteria Validation Cost Check: aws ce get-cost-and-usage \\\r--time-period Start=2024-01-01,End=2024-01-02 \\\r--granularity DAILY \\\r--metrics BlendedCost \\\r--group-by Type=DIMENSION,Key=SERVICE\nPerformance Check:\nMonitor job completion time Check cluster utilization metrics Verify data processing accuracy Reliability Check:\nSimulate spot interruption Verify job recovery Check data consistency K·∫øt lu·∫≠n Ph·∫ßn 2 Nh·ªØng g√¨ ƒë√£ h·ªçc ƒë∆∞·ª£c: ‚úÖ EMR Managed Scaling: Thi·∫øt l·∫≠p v√† c·∫•u h√¨nh ‚úÖ Cost Optimization: Gi·∫£m 40% chi ph√≠ v·ªõi dynamic scaling ‚úÖ Performance Tuning: T·ªëi ∆∞u h√≥a Spark cho auto-scaling ‚úÖ Monitoring: Theo d√µi scaling events v√† metrics ‚úÖ Troubleshooting: X·ª≠ l√Ω common issues ‚úÖ Production Patterns: Best practices cho real-world usage T·ªïng k·∫øt ti·∫øt ki·ªám chi ph√≠: Ph·∫ßn 1 (Spot): 39% savings Ph·∫ßn 2 (Auto Scaling): 40% additional savings Combined: ~65% total cost reduction Next Steps: Trong Ph·∫ßn 3, ch√∫ng ta s·∫Ω thi·∫øt l·∫≠p comprehensive monitoring v√† alerting system ƒë·ªÉ ƒë·∫£m b·∫£o cluster ho·∫°t ƒë·ªông optimal v√† cost-effective.\nPro Tip: Trong production, combine auto-scaling v·ªõi scheduled scaling cho predictable workloads ƒë·ªÉ achieve t·ªëi ƒëa 70-80% cost savings!\nTi·∫øp theo: Ph·∫ßn 3: Monitoring \u0026amp; Alerting ƒë·ªÉ complete cost optimization journey.\n"
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/3-mornitoring/",
	"title": "Ph·∫ßn 3: Gi√°m s√°t &amp; C·∫£nh b√°o",
	"tags": [],
	"description": "",
	"content": "Ph·∫ßn 3: Gi√°m s√°t \u0026amp; C·∫£nh b√°o EMR - Kh·∫£ nƒÉng quan s√°t ho√†n ch·ªânh Trong ph·∫ßn cu·ªëi n√†y, b·∫°n s·∫Ω thi·∫øt l·∫≠p h·ªá th·ªëng gi√°m s√°t v√† c·∫£nh b√°o to√†n di·ªán cho c√°c cluster EMR ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a chi ph√≠ ƒë·ªÉ ƒë·∫£m b·∫£o ch√∫ng ho·∫°t ƒë·ªông hi·ªáu qu·∫£ v√† trong ng√¢n s√°ch ·ªü m√¥i tr∆∞·ªùng production.\nNh·ªØng g√¨ b·∫°n s·∫Ω h·ªçc Gi√°m s√°t th·ªùi gian th·ª±c c√°c cluster EMR v√† chi ph√≠ C·∫£nh b√°o ch·ªß ƒë·ªông cho c√°c v·∫•n ƒë·ªÅ v·ªÅ hi·ªáu su·∫•t v√† chi ph√≠ Dashboard t√πy ch·ªânh cho c√°c b√™n li√™n quan kh√°c nhau Kh·∫Øc ph·ª•c t·ª± ƒë·ªông c√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p Theo d√µi chi ph√≠ v√† t·ªëi ∆∞u h√≥a c√°c khuy·∫øn ngh·ªã [N·ªôi dung ti·∫øng Vi·ªát t∆∞∆°ng t·ª± nh∆∞ b·∫£n ti·∫øng Anh\u0026hellip;]\nK·∫øt lu·∫≠n Nh·ªØng g√¨ b·∫°n ƒë√£ ho√†n th√†nh: ‚úÖ Gi√°m s√°t ho√†n ch·ªânh: Gi√°m s√°t cluster v√† chi ph√≠ th·ªùi gian th·ª±c ‚úÖ C·∫£nh b√°o ch·ªß ƒë·ªông: C·∫£nh b√°o ƒëa c·∫•p cho c√°c t√¨nh hu·ªëng kh√°c nhau ‚úÖ Kh·∫Øc ph·ª•c t·ª± ƒë·ªông: Kh·∫£ nƒÉng t·ª± ph·ª•c h·ªìi cho c√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p ‚úÖ Theo d√µi chi ph√≠: Ph√¢n t√≠ch chi ph√≠ chi ti·∫øt v√† qu·∫£n l√Ω ng√¢n s√°ch ‚úÖ S·∫µn s√†ng production: Tri·ªÉn khai Lambda v√† l·∫≠p l·ªãch T√≥m t·∫Øt ti·∫øt ki·ªám chi ph√≠ cu·ªëi c√πng: Ph·∫ßn 1 (Spot Instances): Ti·∫øt ki·ªám 39% Ph·∫ßn 2 (Auto Scaling): Ti·∫øt ki·ªám th√™m 40% Ph·∫ßn 3 (Monitoring): Ti·∫øt ki·ªám th√™m 15% th√¥ng qua t·ªëi ∆∞u h√≥a T·ªïng ti·∫øt ki·ªám k·∫øt h·ª£p: Gi·∫£m chi ph√≠ ~70% T√°c ƒë·ªông th·ª±c t·∫ø: C√¥ng ty nh·ªè: $2,000/th√°ng ‚Üí $600/th√°ng = Ti·∫øt ki·ªám $1,400 C√¥ng ty trung b√¨nh: $10,000/th√°ng ‚Üí $3,000/th√°ng = Ti·∫øt ki·ªám $7,000 Doanh nghi·ªáp l·ªõn: $40,000/th√°ng ‚Üí $12,000/th√°ng = Ti·∫øt ki·ªám $28,000 C√°c ch·ªâ s·ªë th√†nh c√¥ng ch√≠nh: Gi·∫£m chi ph√≠: ƒê·∫°t ƒë∆∞·ª£c m·ª©c ti·∫øt ki·ªám trung b√¨nh 70% ƒê·ªô tin c·∫≠y: 99.9% uptime v·ªõi kh·∫Øc ph·ª•c t·ª± ƒë·ªông Hi·ªáu su·∫•t: Duy tr√¨ ho·∫∑c c·∫£i thi·ªán th·ªùi gian ho√†n th√†nh job Hi·ªáu qu·∫£ v·∫≠n h√†nh: Gi·∫£m 80% can thi·ªáp th·ªß c√¥ng M·∫πo chuy√™n nghi·ªáp: H·ªá th·ªëng gi√°m s√°t t·ª± ho√†n v·ªën ch·ªâ b·∫±ng vi·ªác ngƒÉn ch·∫∑n m·ªôt l·∫ßn v∆∞·ª£t chi ph√≠ l·ªõn ho·∫∑c s·ª± c·ªë hi·ªáu su·∫•t!\nCh√∫c m·ª´ng! B·∫°n ƒë√£ ho√†n th√†nh Workshop t·ªëi ∆∞u h√≥a chi ph√≠ EMR to√†n di·ªán. C√°c cluster c·ªßa b·∫°n hi·ªán ƒëang ch·∫°y v·ªõi hi·ªáu qu·∫£ t·ªëi ƒëa, chi ph√≠ t·ªëi thi·ªÉu v√† kh·∫£ nƒÉng quan s√°t ƒë·∫ßy ƒë·ªß.\nC√°c b∆∞·ªõc ti·∫øp theo: √Åp d·ª•ng c√°c m·∫´u n√†y cho workload production c·ªßa b·∫°n v√† t·∫≠n h∆∞·ªüng vi·ªác ti·∫øt ki·ªám chi ph√≠ ƒë√°ng k·ªÉ!\n"
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/4-workload-scheduling/",
	"title": "Ph·∫ßn 4: D·ªçn d·∫πp t√†i nguy√™n",
	"tags": [],
	"description": "",
	"content": "Ph·∫ßn 4: D·ªçn d·∫πp t√†i nguy√™n - Qu·∫£n l√Ω chi ph√≠ Sau khi ho√†n th√†nh workshop, vi·ªác d·ªçn d·∫πp t·∫•t c·∫£ t√†i nguy√™n l√† r·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh c√°c kho·∫£n ph√≠ kh√¥ng mong mu·ªën. Ph·∫ßn n√†y cung c·∫•p quy tr√¨nh d·ªçn d·∫πp to√†n di·ªán cho t·∫•t c·∫£ t√†i nguy√™n ƒë∆∞·ª£c t·∫°o trong workshop.\nM·ª•c ti√™u d·ªçn d·∫πp Terminate EMR clusters v√† c√°c t√†i nguy√™n li√™n quan X√≥a CloudWatch alarms, dashboards, v√† custom metrics X√≥a SNS topics v√† subscriptions D·ªçn d·∫πp Lambda functions v√† EventBridge rules X√≥a S3 buckets v√† logs (n·∫øu c√≥ t·∫°o) X√°c minh d·ªçn d·∫πp ho√†n t·∫•t ƒë·ªÉ tr√°nh ph√≠ C·∫£nh b√°o quan tr·ªçng QUAN TR·ªåNG: Vi·ªác x√≥a t√†i nguy√™n kh√¥ng th·ªÉ ho√†n t√°c. ƒê·∫£m b·∫£o b·∫°n ƒë√£ sao l∆∞u d·ªØ li·ªáu quan tr·ªçng tr∆∞·ªõc khi ti·∫øn h√†nh.\nT√°c ƒë·ªông chi ph√≠: ƒê·ªÉ t√†i nguy√™n ch·∫°y c√≥ th·ªÉ t·ªën $50-200+ m·ªói ng√†y cho m·ªôt EMR cluster.\n[N·ªôi dung ti·∫øng Vi·ªát t∆∞∆°ng t·ª± nh∆∞ b·∫£n ti·∫øng Anh\u0026hellip;]\nK·∫øt lu·∫≠n Ch√∫c m·ª´ng! B·∫°n ƒë√£ ho√†n th√†nh vi·ªác d·ªçn d·∫πp t√†i nguy√™n m·ªôt c√°ch an to√†n.\nChecklist cu·ªëi c√πng: T·∫•t c·∫£ EMR clusters ƒë√£ terminate CloudWatch alarms v√† dashboards ƒë√£ x√≥a SNS topics ƒë√£ x√≥a Lambda functions ƒë√£ x√≥a S3 buckets ƒë√£ d·ªçn d·∫πp Ki·ªÉm tra AWS Console "
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/000058-SessionManager/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]