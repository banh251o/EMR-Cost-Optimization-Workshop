<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head><script src="/000058-SessionManager/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=000058-SessionManager/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="description" content="">
<meta name="author" content="baoanh25100@gmail.com">

    <link rel="icon" href="/000058-SessionManager/images/favicon.png" type="image/png">

    <title>Part 2: Auto Scaling :: AWS System Manager</title>

    
    <link href="/000058-SessionManager/css/nucleus.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/fontawesome-all.min.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/hybrid.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/featherlight.min.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/perfect-scrollbar.min.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/auto-complete.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/atom-one-dark-reasonable.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/theme.css?1755036723" rel="stylesheet">
    <link href="/000058-SessionManager/css/hugo-theme.css?1755036723" rel="stylesheet">
    
    <link href="/000058-SessionManager/css/theme-workshop.css?1755036723" rel="stylesheet">
    
    

    <script src="/000058-SessionManager/js/jquery-3.3.1.min.js?1755036723"></script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    
  </head>
  <body class="" data-url="/000058-SessionManager/2-auto-scaling/">
    <nav id="sidebar" class="showVisitedLinks">



  <div id="header-wrapper">
    <div id="header">
      <a id="logo" href="/">

<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff;}.cls-2{fill:#f90;fill-rule:evenodd;}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09,10.85a4.7,4.7,0,0,0,.19,1.48,7.73,7.73,0,0,0,.54,1.19.77.77,0,0,1,.12.38.64.64,0,0,1-.32.49l-1,.7a.83.83,0,0,1-.44.15.69.69,0,0,1-.49-.23,3.8,3.8,0,0,1-.6-.77q-.25-.42-.51-1a6.14,6.14,0,0,1-4.89,2.3,4.54,4.54,0,0,1-3.32-1.19,4.27,4.27,0,0,1-1.22-3.2A4.28,4.28,0,0,1,3.61,7.75,6.06,6.06,0,0,1,7.69,6.46a12.47,12.47,0,0,1,1.76.13q.92.13,1.91.36V5.73a3.65,3.65,0,0,0-.79-2.66A3.81,3.81,0,0,0,7.86,2.3a7.71,7.71,0,0,0-1.79.22,12.78,12.78,0,0,0-1.79.57,4.55,4.55,0,0,1-.58.22l-.26,0q-.35,0-.35-.52V2a1.09,1.09,0,0,1,.12-.58,1.2,1.2,0,0,1,.47-.35A10.88,10.88,0,0,1,5.77.32,10.19,10.19,0,0,1,8.36,0a6,6,0,0,1,4.35,1.35,5.49,5.49,0,0,1,1.38,4.09ZM7.34,13.38a5.36,5.36,0,0,0,1.72-.31A3.63,3.63,0,0,0,10.63,12,2.62,2.62,0,0,0,11.19,11a5.63,5.63,0,0,0,.16-1.44v-.7a14.35,14.35,0,0,0-1.53-.28,12.37,12.37,0,0,0-1.56-.1,3.84,3.84,0,0,0-2.47.67A2.34,2.34,0,0,0,5,11a2.35,2.35,0,0,0,.61,1.76A2.4,2.4,0,0,0,7.34,13.38Zm13.35,1.8a1,1,0,0,1-.64-.16,1.3,1.3,0,0,1-.35-.65L15.81,1.51a3,3,0,0,1-.15-.67.36.36,0,0,1,.41-.41H17.7a1,1,0,0,1,.65.16,1.4,1.4,0,0,1,.33.65l2.79,11,2.59-11A1.17,1.17,0,0,1,24.39.6a1.1,1.1,0,0,1,.67-.16H26.4a1.1,1.1,0,0,1,.67.16,1.17,1.17,0,0,1,.32.65L30,12.39,32.88,1.25A1.39,1.39,0,0,1,33.22.6a1,1,0,0,1,.65-.16h1.54a.36.36,0,0,1,.41.41,1.36,1.36,0,0,1,0,.26,3.64,3.64,0,0,1-.12.41l-4,12.86a1.3,1.3,0,0,1-.35.65,1,1,0,0,1-.64.16H29.25a1,1,0,0,1-.67-.17,1.26,1.26,0,0,1-.32-.67L25.67,3.64,23.11,14.34a1.26,1.26,0,0,1-.32.67,1,1,0,0,1-.67.17Zm21.36.44a11.28,11.28,0,0,1-2.56-.29,7.44,7.44,0,0,1-1.92-.67,1,1,0,0,1-.61-.93v-.84q0-.52.38-.52a.9.9,0,0,1,.31.06l.42.17a8.77,8.77,0,0,0,1.83.58,9.78,9.78,0,0,0,2,.2,4.48,4.48,0,0,0,2.43-.55,1.76,1.76,0,0,0,.86-1.57,1.61,1.61,0,0,0-.45-1.16A4.29,4.29,0,0,0,43,9.22l-2.41-.76A5.15,5.15,0,0,1,38,6.78a3.94,3.94,0,0,1-.83-2.41,3.7,3.7,0,0,1,.45-1.85,4.47,4.47,0,0,1,1.19-1.37A5.27,5.27,0,0,1,40.51.29,7.4,7.4,0,0,1,42.6,0a8.87,8.87,0,0,1,1.12.07q.57.07,1.08.19t.95.26a4.27,4.27,0,0,1,.7.29,1.59,1.59,0,0,1,.49.41.94.94,0,0,1,.15.55v.79q0,.52-.38.52a1.76,1.76,0,0,1-.64-.2,7.74,7.74,0,0,0-3.2-.64,4.37,4.37,0,0,0-2.21.47,1.6,1.6,0,0,0-.79,1.48,1.58,1.58,0,0,0,.49,1.18,4.94,4.94,0,0,0,1.83.92L44.55,7a5.08,5.08,0,0,1,2.57,1.6A3.76,3.76,0,0,1,47.9,11a4.21,4.21,0,0,1-.44,1.93,4.4,4.4,0,0,1-1.21,1.47,5.43,5.43,0,0,1-1.85.93A8.25,8.25,0,0,1,42.05,15.62Z"></path><path class="cls-2" d="M45.19,23.81C39.72,27.85,31.78,30,25,30A36.64,36.64,0,0,1,.22,20.57c-.51-.46-.06-1.09.56-.74A49.78,49.78,0,0,0,25.53,26.4,49.23,49.23,0,0,0,44.4,22.53C45.32,22.14,46.1,23.14,45.19,23.81Z"></path><path class="cls-2" d="M47.47,21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74,3.13-2.2,8.27-1.57,8.86-.83s-.16,5.89-3.09,8.35c-.45.38-.88.18-.68-.32C46.69,25.8,48.17,22.11,47.47,21.21Z"></path></svg>

</a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="/000058-SessionManager/js/lunr.min.js?1755036723"></script>
<script type="text/javascript" src="/000058-SessionManager/js/auto-complete.js?1755036723"></script>
<script type="text/javascript">
    
        var baseurl = "http:\/\/localhost:1313\/000058-SessionManager\/";
    
</script>
<script type="text/javascript" src="/000058-SessionManager/js/search.js?1755036723"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          




 
  
    
    <li data-nav-id="/000058-SessionManager/1-spot-instances/" title="Part 1: Spot Instances" class="dd-item 
        
        
        
        ">
      <a href="/000058-SessionManager/1-spot-instances/">
          Part 1: Spot Instances
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/000058-SessionManager/2-auto-scaling/" title="Part 2: Auto Scaling" class="dd-item 
        
        active
        
        ">
      <a href="/000058-SessionManager/2-auto-scaling/">
          Part 2: Auto Scaling
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/000058-SessionManager/3-mornitoring/" title="Part 3: Monitoring &amp; Alerting" class="dd-item 
        
        
        
        ">
      <a href="/000058-SessionManager/3-mornitoring/">
          Part 3: Monitoring &amp; Alerting
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/000058-SessionManager/4-workload-scheduling/" title="Part 4: Resource Cleanup" class="dd-item 
        
        
        
        ">
      <a href="/000058-SessionManager/4-workload-scheduling/">
          Part 4: Resource Cleanup
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

          
         
    </ul>

    
    
      <section id="shortcuts">
        <h3>More</h3>
        <ul>
          
              <li> 
                  <a class="padding" href="https://www.facebook.com/groups/awsstudygroupfcj/"><i class='fab fa-facebook'></i> AWS Study Group</a>
              </li>
          
        </ul>
      </section>
    

    
    <section id="prefooter">
      <hr/>
      <ul>
      
        <li>
          <a class="padding">
            <i class="fas fa-language fa-fw"></i>
          <div class="select-style">
            <select id="select-language" onchange="location = this.value;">
          
          
          
              
              
                  
                    
                    
                      <option id="en" value="http://localhost:1313/000058-SessionManager/2-auto-scaling/" selected>English</option>
                    
                  
              
                  
              
          
              
              
                  
              
                  
                    
                    
                      <option id="vi" value="http://localhost:1313/000058-SessionManager/vi/2-auto-scaling/">Tiếng Việt</option>
                    
                  
              
          
        </select>
        <svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
          width="255px" height="255px" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255;" xml:space="preserve">
          <g>
            <g id="arrow-drop-down">
              <polygon points="0,63.75 127.5,191.25 255,63.75 		" />
            </g>
          </g>
        </svg>
        </div>
        </a>
        </li>
      
      
      
        <li><a class="padding" href="#" data-clear-history-toggle=""><i class="fas fa-history fa-fw"></i> Clear History</a></li>
      
      </ul>
    </section>
    
    <section id="footer">
      <left>
    
     <b> Workshop</b> <br>
    <img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title="Migrate" Alt="web counter"   border="0" /></a>  <br>
     <b> <a href="https://cloudjourney.awsstudygroup.com/">Cloud Journey</a></b> <br>
    <img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" Alt="web counter"   border="0"   />
     
</left>
<left>
    <br>
    <br>
        <b> Last Upd </b> <br>
        <i><font color=orange>30-01-2022</font></i>
    </left>
    <left>
        <br>
        <br>
            <b> Team </b> <br>
           
            <i> <a href="https://www.linkedin.com/in/sutrinh/"  style="color:orange">Sử Trịnh  </a> <br>
                <a href="https://www.linkedin.com/in/jotaguy"  style="color:orange">Gia Hưng </a> <br>
                <a href="https://www.linkedin.com/in/hiepnguyendt"  style="color:orange">Thanh Hiệp </a>
               
        </i>
        </left>

<script async defer src="https://buttons.github.io/buttons.js"></script>

    </section>
  </div>
</nav>




        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            <a href='/000058-SessionManager/'>EMR Cost Optimization Workshop</a> > Part 2: Auto Scaling
          
        
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-emr-auto-scaling">What is EMR Auto Scaling?</a></li>
    <li><a href="#types-of-auto-scaling">Types of Auto Scaling</a>
      <ul>
        <li><a href="#1-emr-managed-scaling-recommended">1. EMR Managed Scaling (Recommended)</a></li>
        <li><a href="#2-custom-auto-scaling">2. Custom Auto Scaling</a></li>
      </ul>
    </li>
    <li><a href="#hands-on-setting-up-managed-scaling">Hands-on: Setting up Managed Scaling</a>
      <ul>
        <li><a href="#step-1-enable-managed-scaling">Step 1: Enable Managed Scaling</a></li>
        <li><a href="#step-2-configure-advanced-settings">Step 2: Configure Advanced Settings</a></li>
        <li><a href="#step-3-verify-configuration">Step 3: Verify Configuration</a></li>
      </ul>
    </li>
    <li><a href="#testing-auto-scaling">Testing Auto Scaling</a>
      <ul>
        <li><a href="#create-workload-to-test-scaling">Create Workload to Test Scaling</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#monitor-scaling-process">Monitor Scaling Process</a></li>
      </ul>
    </li>
    <li><a href="#monitoring-and-troubleshooting">Monitoring and Troubleshooting</a>
      <ul>
        <li><a href="#key-metrics-to-monitor">Key Metrics to Monitor</a></li>
        <li><a href="#common-issues-and-solutions">Common Issues and Solutions</a></li>
      </ul>
    </li>
    <li><a href="#advanced-scaling-strategies">Advanced Scaling Strategies</a>
      <ul>
        <li><a href="#1-mixed-instance-types-for-scaling">1. Mixed Instance Types for Scaling</a></li>
        <li><a href="#2-time-based-scaling">2. Time-based Scaling</a></li>
        <li><a href="#3-predictive-scaling">3. Predictive Scaling</a></li>
      </ul>
    </li>
    <li><a href="#cost-optimization-with-auto-scaling">Cost Optimization with Auto Scaling</a>
      <ul>
        <li><a href="#before-auto-scaling-static-cluster">Before Auto Scaling (Static Cluster)</a></li>
        <li><a href="#after-auto-scaling-dynamic-cluster">After Auto Scaling (Dynamic Cluster)</a></li>
      </ul>
    </li>
    <li><a href="#lab-exercise-custom-scaling-policy">Lab Exercise: Custom Scaling Policy</a>
      <ul>
        <li><a href="#create-custom-cloudwatch-alarm">Create Custom CloudWatch Alarm</a></li>
        <li><a href="#test-custom-scaling">Test Custom Scaling</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#production-best-practices">Production Best Practices</a>
      <ul>
        <li><a href="#1-scaling-configuration">1. Scaling Configuration</a></li>
        <li><a href="#2-instance-mix-strategy">2. Instance Mix Strategy</a></li>
        <li><a href="#3-application-design">3. Application Design</a></li>
        <li><a href="#4-monitoring-setup">4. Monitoring Setup</a></li>
      </ul>
    </li>
    <li><a href="#part-2-results">Part 2 Results</a></li>
    <li><a href="#troubleshooting-common-issues">Troubleshooting Common Issues</a>
      <ul>
        <li><a href="#issue-scaling-events-not-appearing">Issue: Scaling Events Not Appearing</a></li>
        <li><a href="#issue-scale-in-too-aggressive">Issue: Scale-in Too Aggressive</a></li>
        <li><a href="#issue-spot-instances-not-added-during-scaling">Issue: Spot Instances Not Added During Scaling</a></li>
      </ul>
    </li>
    <li><a href="#performance-tuning-tips">Performance Tuning Tips</a>
      <ul>
        <li><a href="#1-optimize-spark-configuration">1. Optimize Spark Configuration</a></li>
        <li><a href="#2-yarn-configuration">2. YARN Configuration</a></li>
        <li><a href="#3-emr-steps-optimization">3. EMR Steps Optimization</a></li>
      </ul>
    </li>
    <li><a href="#real-world-example-e-commerce-analytics">Real-world Example: E-commerce Analytics</a>
      <ul>
        <li><a href="#scenario">Scenario</a></li>
        <li><a href="#auto-scaling-configuration">Auto Scaling Configuration</a></li>
        <li><a href="#cost-comparison">Cost Comparison</a></li>
      </ul>
    </li>
    <li><a href="#advanced-monitoring-setup">Advanced Monitoring Setup</a>
      <ul>
        <li><a href="#custom-metrics-dashboard">Custom Metrics Dashboard</a></li>
        <li><a href="#automated-alerts">Automated Alerts</a></li>
        <li><a href="#cost-tracking-script">Cost Tracking Script</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#scaling-patterns-analysis">Scaling Patterns Analysis</a>
      <ul>
        <li><a href="#pattern-1-batch-processing">Pattern 1: Batch Processing</a></li>
        <li><a href="#pattern-2-interactive-analytics">Pattern 2: Interactive Analytics</a></li>
        <li><a href="#pattern-3-streaming-workloads">Pattern 3: Streaming Workloads</a></li>
      </ul>
    </li>
    <li><a href="#integration-with-other-aws-services">Integration with Other AWS Services</a>
      <ul>
        <li><a href="#1-lambda-triggers">1. Lambda Triggers</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#2-step-functions-orchestration">2. Step Functions Orchestration</a></li>
        <li><a href="#3-eventbridge-rules">3. EventBridge Rules</a></li>
      </ul>
    </li>
    <li><a href="#final-lab-end-to-end-scenario">Final Lab: End-to-End Scenario</a>
      <ul>
        <li><a href="#scenario-setup">Scenario Setup</a></li>
        <li><a href="#implementation-steps">Implementation Steps</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#expected-timeline">Expected Timeline</a></li>
      </ul>
    </li>
    <li><a href="#performance-metrics-analysis">Performance Metrics Analysis</a>
      <ul>
        <li><a href="#key-performance-indicators-kpis">Key Performance Indicators (KPIs)</a></li>
        <li><a href="#benchmarking-results">Benchmarking Results</a></li>
      </ul>
    </li>
    <li><a href="#graduation-exercise">Graduation Exercise</a>
      <ul>
        <li><a href="#challenge-optimize-real-workload">Challenge: Optimize Real Workload</a></li>
        <li><a href="#solution-approach">Solution Approach</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#success-criteria-validation">Success Criteria Validation</a></li>
      </ul>
    </li>
    <li><a href="#advanced-auto-scaling-techniques">Advanced Auto Scaling Techniques</a>
      <ul>
        <li><a href="#1-predictive-scaling-with-machine-learning">1. Predictive Scaling with Machine Learning</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#2-multi-cluster-auto-scaling">2. Multi-Cluster Auto Scaling</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#3-cost-aware-scaling">3. Cost-Aware Scaling</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#production-deployment-checklist">Production Deployment Checklist</a>
      <ul>
        <li><a href="#pre-deployment">Pre-Deployment</a></li>
        <li><a href="#scaling-configuration">Scaling Configuration</a></li>
        <li><a href="#monitoring-setup">Monitoring Setup</a></li>
        <li><a href="#testing-validation">Testing Validation</a></li>
      </ul>
    </li>
    <li><a href="#conclusion-part-2">Conclusion Part 2</a>
      <ul>
        <li><a href="#what-youve-accomplished">What You&rsquo;ve Accomplished:</a></li>
        <li><a href="#cost-savings-summary">Cost Savings Summary:</a></li>
        <li><a href="#key-takeaways">Key Takeaways:</a></li>
        <li><a href="#real-world-impact">Real-World Impact:</a></li>
        <li><a href="#next-steps">Next Steps:</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Part 2: Auto Scaling
            </h1>
          

        



	<h1 id="part-2-emr-auto-scaling---dynamic-workload-adjustment">Part 2: EMR Auto Scaling - Dynamic Workload Adjustment</h1>
<p>In this section, you&rsquo;ll learn how to set up EMR Auto Scaling to automatically adjust the number of instances in your cluster based on workload, optimizing both cost and performance.</p>
<h2 id="what-is-emr-auto-scaling">What is EMR Auto Scaling?</h2>
<p>EMR Auto Scaling automatically adjusts the number of instances in your cluster based on:</p>
<ul>
<li><strong>YARN metrics</strong>: Memory and CPU utilization</li>
<li><strong>Custom metrics</strong>: Custom CloudWatch metrics</li>
<li><strong>Time-based scaling</strong>: Scheduled scaling patterns</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Cost savings during low workload periods</li>
<li>Improved performance during high workload periods</li>
<li>No manual intervention required</li>
<li>Great integration with Spot Instances</li>
</ul>
<h2 id="types-of-auto-scaling">Types of Auto Scaling</h2>
<h3 id="1-emr-managed-scaling-recommended">1. EMR Managed Scaling (Recommended)</h3>
<ul>
<li>Fully managed by AWS</li>
<li>Based on YARN container pending metrics</li>
<li>Simple and effective</li>
<li>Supports both On-Demand and Spot instances</li>
</ul>
<h3 id="2-custom-auto-scaling">2. Custom Auto Scaling</h3>
<ul>
<li>Define your own scaling policies</li>
<li>Based on CloudWatch metrics</li>
<li>More flexible but complex</li>
<li>Suitable for special use cases</li>
</ul>
<h2 id="hands-on-setting-up-managed-scaling">Hands-on: Setting up Managed Scaling</h2>
<h3 id="step-1-enable-managed-scaling">Step 1: Enable Managed Scaling</h3>
<p>Using the cluster from Part 1, we&rsquo;ll enable auto scaling:</p>
<ol>
<li>Open EMR Console</li>
<li>Select cluster &ldquo;Workshop-Spot-Cluster&rdquo;</li>
<li>Go to &ldquo;Configuration&rdquo; tab</li>
<li>Click &ldquo;Edit&rdquo; in the &ldquo;Scaling&rdquo; section</li>
</ol>
<p><strong>Managed Scaling Configuration:</strong></p>
<ul>
<li>Minimum capacity: 2 instances</li>
<li>Maximum capacity: 10 instances</li>
<li>Maximum On-Demand capacity: 4 instances</li>
</ul>
<h3 id="step-2-configure-advanced-settings">Step 2: Configure Advanced Settings</h3>
<p><strong>Scale-out settings:</strong></p>
<ul>
<li>Scale out cooldown: 300 seconds</li>
<li>Maximum scale-out increment: 100%</li>
</ul>
<p><strong>Scale-in settings:</strong></p>
<ul>
<li>Scale in cooldown: 300 seconds</li>
<li>Maximum scale-in increment: 50%</li>
</ul>
<h3 id="step-3-verify-configuration">Step 3: Verify Configuration</h3>
<p>After enabling, check:</p>
<ul>
<li>Scaling status: &ldquo;Enabled&rdquo;</li>
<li>Current capacity: 7 instances (from Part 1)</li>
<li>Target capacity: Will change based on workload</li>
</ul>
<h2 id="testing-auto-scaling">Testing Auto Scaling</h2>
<h3 id="create-workload-to-test-scaling">Create Workload to Test Scaling</h3>
<ol>
<li>
<p><strong>SSH to Master Node:</strong>
<code>ssh -i your-key.pem hadoop@master-public-ip</code></p>
</li>
<li>
<p><strong>Create Test Script:</strong>
``python</p>
</li>
</ol>
<h1 id="scaling-testpy">scaling-test.py</h1>
<p>from pyspark.sql import SparkSession
import time</p>
<p>spark = SparkSession.builder <br>
.appName(&ldquo;AutoScalingTest&rdquo;) <br>
.config(&ldquo;spark.sql.adaptive.enabled&rdquo;, &ldquo;false&rdquo;) <br>
.config(&ldquo;spark.sql.adaptive.coalescePartitions.enabled&rdquo;, &ldquo;false&rdquo;) <br>
.getOrCreate()</p>
<p>print(&quot;=== Starting Auto Scaling Test ===&quot;)</p>
<h1 id="create-large-dataset-to-trigger-scaling">Create large dataset to trigger scaling</h1>
<p>print(&ldquo;Creating large dataset&hellip;&rdquo;)
df = spark.range(0, 100000000).toDF(&ldquo;id&rdquo;)
df = df.repartition(200)  # Many partitions to require more resources</p>
<h1 id="cache-to-consume-memory">Cache to consume memory</h1>
<p>df.cache()</p>
<h1 id="run-multiple-operations-to-maintain-load">Run multiple operations to maintain load</h1>
<p>for i in range(5):
print(f&quot;Running operation {i+1}/5&hellip;&quot;)</p>
<pre><code># Heavy computation
result = df.filter(df.id % 2 == 0).count()
print(f&quot;Even numbers count: {result}&quot;)

# Keep load for 5 minutes to observe scaling
time.sleep(300)
</code></pre>
<p>print(&quot;=== Test completed ===&quot;)
spark.stop()
``</p>
<ol start="3">
<li><strong>Submit Job:</strong>
<code>spark-submit \ --executor-memory 2g \ --num-executors 15 \ --executor-cores 2 \ scaling-test.py</code></li>
</ol>
<h3 id="monitor-scaling-process">Monitor Scaling Process</h3>
<p><strong>In EMR Console:</strong></p>
<ol>
<li>Go to &ldquo;Hardware&rdquo; tab to view instances</li>
<li>Refresh every 2-3 minutes</li>
<li>Observe instance count increasing</li>
</ol>
<p><strong>In CloudWatch:</strong></p>
<ol>
<li>Open CloudWatch Console</li>
<li>Go to &ldquo;Metrics&rdquo; → &ldquo;AWS/ElasticMapReduce&rdquo;</li>
<li>Select metrics:
<ul>
<li>YARNMemoryAvailablePercentage</li>
<li>ContainerPending</li>
<li>AppsRunning</li>
</ul>
</li>
</ol>
<p><strong>Expected Behavior:</strong></p>
<ul>
<li><strong>Minutes 0-2</strong>: Job starts, YARN memory decreases</li>
<li><strong>Minutes 2-5</strong>: ContainerPending increases, scaling triggered</li>
<li><strong>Minutes 5-8</strong>: New instances added (2-3 instances)</li>
<li><strong>Minutes 8-25</strong>: Job runs with new capacity</li>
<li><strong>Minutes 25-30</strong>: Job completes, scaling down begins</li>
</ul>
<h2 id="monitoring-and-troubleshooting">Monitoring and Troubleshooting</h2>
<h3 id="key-metrics-to-monitor">Key Metrics to Monitor</h3>
<p><strong>YARN Metrics:</strong></p>
<ul>
<li><strong>YARNMemoryAvailablePercentage</strong>: &lt; 15% triggers scale out</li>
<li><strong>ContainerPending</strong>: &gt; 0 for 5 minutes triggers scale out</li>
<li><strong>AppsRunning</strong>: Number of running applications</li>
</ul>
<p><strong>EMR Metrics:</strong></p>
<ul>
<li><strong>RunningMapTasks</strong>: Running map tasks</li>
<li><strong>RunningReduceTasks</strong>: Running reduce tasks</li>
<li><strong>TotalLoad</strong>: Total cluster load</li>
</ul>
<h3 id="common-issues-and-solutions">Common Issues and Solutions</h3>
<p><strong>Issue 1: Scaling not working</strong></p>
<ul>
<li>Check IAM permissions</li>
<li>Verify scaling limits (min/max capacity)</li>
<li>Check cooldown periods</li>
</ul>
<p><strong>Issue 2: Scale out too slow</strong></p>
<ul>
<li>Reduce scale-out cooldown</li>
<li>Increase maximum scale-out increment</li>
<li>Use multiple instance types</li>
</ul>
<p><strong>Issue 3: Scale in too aggressive</strong></p>
<ul>
<li>Increase scale-in cooldown</li>
<li>Reduce maximum scale-in increment</li>
<li>Adjust YARN memory thresholds</li>
</ul>
<h2 id="advanced-scaling-strategies">Advanced Scaling Strategies</h2>
<h3 id="1-mixed-instance-types-for-scaling">1. Mixed Instance Types for Scaling</h3>
<p>Configure multiple instance types to increase availability:</p>
<p><strong>Instance Fleet Configuration:</strong></p>
<ul>
<li>Primary: m5.large (Spot)</li>
<li>Secondary: m4.large (Spot)</li>
<li>Fallback: c5.large (Spot)</li>
<li>Emergency: m5.large (On-Demand)</li>
</ul>
<h3 id="2-time-based-scaling">2. Time-based Scaling</h3>
<p>For workloads with fixed patterns:</p>
<ul>
<li>Scale out before peak hours</li>
<li>Scale in after off-peak hours</li>
<li>Use CloudWatch Events + Lambda</li>
</ul>
<h3 id="3-predictive-scaling">3. Predictive Scaling</h3>
<p>Based on historical data:</p>
<ul>
<li>Analyze past workload patterns</li>
<li>Pre-scale for expected load</li>
<li>Combine with reactive scaling</li>
</ul>
<h2 id="cost-optimization-with-auto-scaling">Cost Optimization with Auto Scaling</h2>
<h3 id="before-auto-scaling-static-cluster">Before Auto Scaling (Static Cluster)</h3>
<ul>
<li><strong>Peak capacity</strong>: 10 instances × 8 hours = 80 instance-hours</li>
<li><strong>Cost</strong>: 80 × $0.096 = $7.68/day</li>
</ul>
<h3 id="after-auto-scaling-dynamic-cluster">After Auto Scaling (Dynamic Cluster)</h3>
<ul>
<li><strong>Average capacity</strong>: 4 instances × 8 hours = 32 instance-hours</li>
<li><strong>Peak capacity</strong>: 8 instances × 2 hours = 16 instance-hours</li>
<li><strong>Total</strong>: 32 + 16 = 48 instance-hours</li>
<li><strong>Cost</strong>: 48 × $0.096 = $4.61/day</li>
</ul>
<p><strong>Savings</strong>: $3.07/day (40% cost reduction)</p>
<h2 id="lab-exercise-custom-scaling-policy">Lab Exercise: Custom Scaling Policy</h2>
<h3 id="create-custom-cloudwatch-alarm">Create Custom CloudWatch Alarm</h3>
<ol>
<li>
<p><strong>Create Scale-Out Alarm:</strong>
<code>aws cloudwatch put-metric-alarm \ --alarm-name &quot;EMR-ScaleOut-HighMemory&quot; \ --alarm-description &quot;Scale out when memory usage &gt; 80%&quot; \ --metric-name YARNMemoryAvailablePercentage \ --namespace AWS/ElasticMapReduce \ --statistic Average \ --period 300 \ --threshold 20 \ --comparison-operator LessThanThreshold \ --evaluation-periods 2 \ --dimensions Name=JobFlowId,Value=j-xxxxx</code></p>
</li>
<li>
<p><strong>Create Scale-In Alarm:</strong>
<code>aws cloudwatch put-metric-alarm \ --alarm-name &quot;EMR-ScaleIn-LowMemory&quot; \ --alarm-description &quot;Scale in when memory usage &lt; 30%&quot; \ --metric-name YARNMemoryAvailablePercentage \ --namespace AWS/ElasticMapReduce \ --statistic Average \ --period 600 \ --threshold 70 \ --comparison-operator GreaterThanThreshold \ --evaluation-periods 3 \ --dimensions Name=JobFlowId,Value=j-xxxxx</code></p>
</li>
</ol>
<h3 id="test-custom-scaling">Test Custom Scaling</h3>
<ol>
<li><strong>Create Light Workload:</strong>
``python</li>
</ol>
<h1 id="light-workloadpy">light-workload.py</h1>
<p>from pyspark.sql import SparkSession</p>
<p>spark = SparkSession.builder.appName(&ldquo;LightWorkload&rdquo;).getOrCreate()</p>
<h1 id="small-dataset">Small dataset</h1>
<p>df = spark.range(0, 1000000).toDF(&ldquo;id&rdquo;)
result = df.count()
print(f&quot;Light workload result: {result}&quot;)</p>
<p>spark.stop()
``</p>
<ol start="2">
<li>
<p><strong>Submit and Monitor:</strong>
<code>spark-submit light-workload.py</code></p>
</li>
<li>
<p><strong>Observe Scale-In:</strong></p>
</li>
</ol>
<ul>
<li>Memory usage drops below 30%</li>
<li>After 10 minutes, cluster scales in</li>
<li>Instances reduce from 8 to 4</li>
</ul>
<h2 id="production-best-practices">Production Best Practices</h2>
<h3 id="1-scaling-configuration">1. Scaling Configuration</h3>
<p><strong>Recommended Settings:</strong></p>
<ul>
<li>Min capacity: 20% of peak capacity</li>
<li>Max capacity: 150% of expected peak</li>
<li>Scale-out cooldown: 300 seconds</li>
<li>Scale-in cooldown: 600 seconds</li>
</ul>
<h3 id="2-instance-mix-strategy">2. Instance Mix Strategy</h3>
<p><strong>Optimal Mix:</strong></p>
<ul>
<li>30% On-Demand (stability)</li>
<li>70% Spot (cost savings)</li>
<li>Multiple instance families</li>
<li>Diversified AZs</li>
</ul>
<h3 id="3-application-design">3. Application Design</h3>
<p><strong>Scaling-Friendly Applications:</strong></p>
<ul>
<li>Stateless processing</li>
<li>Checkpointing enabled</li>
<li>Graceful handling of node loss</li>
<li>Appropriate data partitioning</li>
</ul>
<h3 id="4-monitoring-setup">4. Monitoring Setup</h3>
<p><strong>Essential Metrics:</strong></p>
<ul>
<li>Cluster utilization</li>
<li>Scaling events</li>
<li>Job completion times</li>
<li>Cost per job</li>
</ul>
<h2 id="part-2-results">Part 2 Results</h2>
<p>After completing this section, you have:</p>
<ul>
<li>✅ Successfully set up EMR Managed Scaling</li>
<li>✅ Tested scaling with real workload</li>
<li>✅ Understood how to monitor scaling metrics</li>
<li>✅ Optimized an additional 40% cost with dynamic scaling</li>
</ul>
<p><strong>Total savings so far:</strong></p>
<ul>
<li>Spot Instances: 39% (from Part 1)</li>
<li>Auto Scaling: 40% (from Part 2)</li>
<li><strong>Combined savings</strong>: ~65% compared to static On-Demand cluster</li>
</ul>

<div class="notices success" ><p><strong>Excellent!</strong> Your cluster now automatically scales based on workload and maximizes cost savings. Next, we&rsquo;ll set up monitoring to track everything.</p>
</div>

<h2 id="troubleshooting-common-issues">Troubleshooting Common Issues</h2>
<h3 id="issue-scaling-events-not-appearing">Issue: Scaling Events Not Appearing</h3>
<p><strong>Possible Causes:</strong></p>
<ul>
<li>IAM role missing permissions</li>
<li>Cooldown period not expired</li>
<li>Metrics not reaching threshold</li>
</ul>
<p><strong>Solutions:</strong></p>
<ol>
<li>Check CloudTrail logs</li>
<li>Verify EMR service role permissions</li>
<li>Adjust threshold values</li>
</ol>
<h3 id="issue-scale-in-too-aggressive">Issue: Scale-in Too Aggressive</h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Instances terminated while jobs running</li>
<li>Performance degradation</li>
</ul>
<p><strong>Solutions:</strong></p>
<ol>
<li>Increase scale-in cooldown to 900s</li>
<li>Reduce maximum scale-in increment to 25%</li>
<li>Set higher memory threshold (80% instead of 70%)</li>
</ol>
<h3 id="issue-spot-instances-not-added-during-scaling">Issue: Spot Instances Not Added During Scaling</h3>
<p><strong>Causes:</strong></p>
<ul>
<li>Spot capacity not available</li>
<li>Bid price too low</li>
<li>Instance type constraints</li>
</ul>
<p><strong>Solutions:</strong></p>
<ol>
<li>Add multiple instance types</li>
<li>Increase bid price</li>
<li>Enable multiple AZs</li>
</ol>
<h2 id="performance-tuning-tips">Performance Tuning Tips</h2>
<h3 id="1-optimize-spark-configuration">1. Optimize Spark Configuration</h3>
<p><code># Spark configs for auto-scaling environment spark.dynamicAllocation.enabled=true spark.dynamicAllocation.minExecutors=2 spark.dynamicAllocation.maxExecutors=50 spark.dynamicAllocation.initialExecutors=5</code></p>
<h3 id="2-yarn-configuration">2. YARN Configuration</h3>
<p>``xml</p>
<!-- raw HTML omitted -->
<h3 id="3-emr-steps-optimization">3. EMR Steps Optimization</h3>
<ul>
<li>Use cluster mode instead of client mode</li>
<li>Enable speculation for fault tolerance</li>
<li>Configure appropriate parallelism</li>
</ul>
<h2 id="real-world-example-e-commerce-analytics">Real-world Example: E-commerce Analytics</h2>
<h3 id="scenario">Scenario</h3>
<p>E-commerce company needs to process daily log data:</p>
<ul>
<li><strong>Morning (6-10 AM)</strong>: Light processing (2-3 instances)</li>
<li><strong>Afternoon (2-6 PM)</strong>: Heavy analytics (8-12 instances)</li>
<li><strong>Night (10 PM-2 AM)</strong>: Batch reports (4-6 instances)</li>
</ul>
<h3 id="auto-scaling-configuration">Auto Scaling Configuration</h3>
<p><code># Managed scaling policy { &quot;ComputeLimits&quot;: { &quot;UnitType&quot;: &quot;Instances&quot;, &quot;MinimumCapacityUnits&quot;: 2, &quot;MaximumCapacityUnits&quot;: 15, &quot;MaximumOnDemandCapacityUnits&quot;: 5, &quot;MaximumCoreCapacityUnits&quot;: 8 } }</code></p>
<h3 id="cost-comparison">Cost Comparison</h3>
<p><strong>Before Auto Scaling:</strong></p>
<ul>
<li>Static 12 instances × 24 hours = 288 instance-hours</li>
<li>Cost: 288 × $0.096 = $27.65/day</li>
</ul>
<p><strong>After Auto Scaling:</strong></p>
<ul>
<li>Average 5 instances × 24 hours = 120 instance-hours</li>
<li>Cost: 120 × $0.096 = $11.52/day</li>
<li><strong>Savings: $16.13/day (58%)</strong></li>
</ul>
<h2 id="advanced-monitoring-setup">Advanced Monitoring Setup</h2>
<h3 id="custom-metrics-dashboard">Custom Metrics Dashboard</h3>
<p>Create CloudWatch dashboard with:</p>
<ul>
<li>Cluster capacity over time</li>
<li>Cost per hour tracking</li>
<li>Job completion rates</li>
<li>Scaling events timeline</li>
</ul>
<h3 id="automated-alerts">Automated Alerts</h3>
<p>Setup alerts for:</p>
<ul>
<li>Scaling failures</li>
<li>High cost thresholds</li>
<li>Performance degradation</li>
<li>Spot interruption rates</li>
</ul>
<h3 id="cost-tracking-script">Cost Tracking Script</h3>
<p>``python</p>
<h1 id="cost-trackerpy">cost-tracker.py</h1>
<p>import boto3
from datetime import datetime, timedelta</p>
<p>def track_emr_costs(cluster_id):
emr = boto3.client(&rsquo;emr&rsquo;)
ce = boto3.client(&lsquo;ce&rsquo;)</p>
<pre><code># Get cluster info
cluster = emr.describe_cluster(ClusterId=cluster_id)
start_time = cluster['Cluster']['Status']['Timeline']['CreationDateTime']

# Calculate cost
end_time = datetime.now()

response = ce.get_cost_and_usage(
    TimePeriod={
        'Start': start_time.strftime('%Y-%m-%d'),
        'End': end_time.strftime('%Y-%m-%d')
    },
    Granularity='DAILY',
    Metrics=['BlendedCost'],
    GroupBy=[
        {'Type': 'DIMENSION', 'Key': 'SERVICE'}
    ]
)

print(f&quot;Cluster {cluster_id} cost tracking:&quot;)
for result in response['ResultsByTime']:
    for group in result['Groups']:
        if 'ElasticMapReduce' in group['Keys'][0]:
            cost = group['Metrics']['BlendedCost']['Amount']
            print(f&quot;Date: {result['TimePeriod']['Start']}, Cost: ${cost}&quot;)
</code></pre>
<h1 id="usage">Usage</h1>
<p>track_emr_costs(&lsquo;j-xxxxx&rsquo;)
``</p>
<h2 id="scaling-patterns-analysis">Scaling Patterns Analysis</h2>
<h3 id="pattern-1-batch-processing">Pattern 1: Batch Processing</h3>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Predictable workload times</li>
<li>High resource usage during processing</li>
<li>Idle periods between jobs</li>
</ul>
<p><strong>Optimal Strategy:</strong></p>
<ul>
<li>Aggressive scale-out (200% increment)</li>
<li>Conservative scale-in (25% increment)</li>
<li>Longer cooldown periods (600s)</li>
</ul>
<h3 id="pattern-2-interactive-analytics">Pattern 2: Interactive Analytics</h3>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Unpredictable query patterns</li>
<li>Variable resource requirements</li>
<li>Need for quick response times</li>
</ul>
<p><strong>Optimal Strategy:</strong></p>
<ul>
<li>Moderate scale-out (100% increment)</li>
<li>Quick scale-in (50% increment)</li>
<li>Shorter cooldown periods (300s)</li>
</ul>
<h3 id="pattern-3-streaming-workloads">Pattern 3: Streaming Workloads</h3>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Continuous data processing</li>
<li>Steady resource usage</li>
<li>Occasional spikes</li>
</ul>
<p><strong>Optimal Strategy:</strong></p>
<ul>
<li>Conservative scaling (50% increment)</li>
<li>Maintain minimum baseline</li>
<li>Focus on stability over cost</li>
</ul>
<h2 id="integration-with-other-aws-services">Integration with Other AWS Services</h2>
<h3 id="1-lambda-triggers">1. Lambda Triggers</h3>
<p>Automatically start/stop clusters:
``python</p>
<h1 id="lambda-emr-schedulerpy">lambda-emr-scheduler.py</h1>
<p>import boto3
import json</p>
<p>def lambda_handler(event, context):
emr = boto3.client(&rsquo;emr&rsquo;)</p>
<pre><code>if event['action'] == 'start':
    # Start cluster with auto-scaling
    response = emr.run_job_flow(
        Name='Scheduled-Cluster',
        ReleaseLabel='emr-6.15.0',
        Instances={
            'MasterInstanceType': 'm5.xlarge',
            'SlaveInstanceType': 'm5.large',
            'InstanceCount': 3,
            'Ec2KeyName': 'your-key'
        },
        Applications=[{'Name': 'Spark'}],
        ServiceRole='EMR_DefaultRole',
        JobFlowRole='EMR_EC2_DefaultRole'
    )
    
    cluster_id = response['JobFlowId']
    
    # Enable managed scaling
    emr.put_managed_scaling_policy(
        ClusterId=cluster_id,
        ManagedScalingPolicy={
            'ComputeLimits': {
                'UnitType': 'Instances',
                'MinimumCapacityUnits': 2,
                'MaximumCapacityUnits': 10
            }
        }
    )
    
    return {'statusCode': 200, 'body': f'Started cluster: {cluster_id}'}

elif event['action'] == 'stop':
    # Terminate cluster
    emr.terminate_job_flows(JobFlowIds=[event['cluster_id']])
    return {'statusCode': 200, 'body': 'Cluster terminated'}
</code></pre>
<p>``</p>
<h3 id="2-step-functions-orchestration">2. Step Functions Orchestration</h3>
<p>Workflow for complex data pipelines:
<code>json { &quot;Comment&quot;: &quot;EMR Auto-scaling Pipeline&quot;, &quot;StartAt&quot;: &quot;CreateCluster&quot;, &quot;States&quot;: { &quot;CreateCluster&quot;: { &quot;Type&quot;: &quot;Task&quot;, &quot;Resource&quot;: &quot;arn:aws:states:::emr:createCluster.sync&quot;, &quot;Parameters&quot;: { &quot;Name&quot;: &quot;Pipeline-Cluster&quot;, &quot;ReleaseLabel&quot;: &quot;emr-6.15.0&quot; }, &quot;Next&quot;: &quot;EnableScaling&quot; }, &quot;EnableScaling&quot;: { &quot;Type&quot;: &quot;Task&quot;, &quot;Resource&quot;: &quot;arn:aws:states:::aws-sdk:emr:putManagedScalingPolicy&quot;, &quot;Parameters&quot;: { &quot;ClusterId.$&quot;: &quot;$.ClusterId&quot;, &quot;ManagedScalingPolicy&quot;: { &quot;ComputeLimits&quot;: { &quot;UnitType&quot;: &quot;Instances&quot;, &quot;MinimumCapacityUnits&quot;: 2, &quot;MaximumCapacityUnits&quot;: 20 } } }, &quot;Next&quot;: &quot;ProcessData&quot; }, &quot;ProcessData&quot;: { &quot;Type&quot;: &quot;Task&quot;, &quot;Resource&quot;: &quot;arn:aws:states:::emr:addStep.sync&quot;, &quot;End&quot;: true } } } </code></p>
<h3 id="3-eventbridge-rules">3. EventBridge Rules</h3>
<p>Automatically respond to scaling events:
<code>json { &quot;Rules&quot;: [ { &quot;Name&quot;: &quot;EMR-ScaleOut-Alert&quot;, &quot;EventPattern&quot;: { &quot;source&quot;: [&quot;aws.emr&quot;], &quot;detail-type&quot;: [&quot;EMR Instance Group State Change&quot;], &quot;detail&quot;: { &quot;state&quot;: [&quot;RUNNING&quot;], &quot;requestedInstanceCount&quot;: { &quot;numeric&quot;: [&quot;&gt;&quot;, 5] } } }, &quot;Targets&quot;: [ { &quot;Id&quot;: &quot;1&quot;, &quot;Arn&quot;: &quot;arn:aws:sns:us-east-1:123456789012:emr-alerts&quot; } ] } ] } </code></p>
<h2 id="final-lab-end-to-end-scenario">Final Lab: End-to-End Scenario</h2>
<h3 id="scenario-setup">Scenario Setup</h3>
<p>Create a complete data processing pipeline:</p>
<ol>
<li><strong>Data Ingestion</strong>: S3 → EMR</li>
<li><strong>Processing</strong>: Spark jobs with auto-scaling</li>
<li><strong>Output</strong>: Results → S3</li>
<li><strong>Monitoring</strong>: CloudWatch dashboard</li>
<li><strong>Cleanup</strong>: Automatic termination</li>
</ol>
<h3 id="implementation-steps">Implementation Steps</h3>
<ol>
<li>
<p><strong>Upload Sample Data:</strong>
<code>aws s3 cp sample-data.csv s3://your-bucket/input/</code></p>
</li>
<li>
<p><strong>Create Processing Script:</strong>
``python</p>
</li>
</ol>
<h1 id="end-to-end-pipelinepy">end-to-end-pipeline.py</h1>
<p>from pyspark.sql import SparkSession
from pyspark.sql.functions import *</p>
<p>spark = SparkSession.builder.appName(&ldquo;E2EPipeline&rdquo;).getOrCreate()</p>
<h1 id="read-data-from-s3">Read data from S3</h1>
<p>df = spark.read.csv(&ldquo;s3://your-bucket/input/sample-data.csv&rdquo;, header=True)</p>
<h1 id="heavy-processing-to-trigger-scaling">Heavy processing to trigger scaling</h1>
<p>df_processed = df.groupBy(&ldquo;category&rdquo;).agg(
count(&quot;*&quot;).alias(&ldquo;count&rdquo;),
avg(&ldquo;value&rdquo;).alias(&ldquo;avg_value&rdquo;),
max(&ldquo;value&rdquo;).alias(&ldquo;max_value&rdquo;)
).repartition(50)  # Force many partitions</p>
<h1 id="cache-to-consume-memory-1">Cache to consume memory</h1>
<p>df_processed.cache()</p>
<h1 id="multiple-operations">Multiple operations</h1>
<p>result1 = df_processed.filter(col(&ldquo;count&rdquo;) &gt; 100).count()
result2 = df_processed.orderBy(desc(&ldquo;avg_value&rdquo;)).collect()</p>
<h1 id="write-results">Write results</h1>
<p>df_processed.write.mode(&ldquo;overwrite&rdquo;).csv(&ldquo;s3://your-bucket/output/&rdquo;)</p>
<p>print(f&quot;Pipeline completed. Processed {result1} categories&quot;)
spark.stop()
``</p>
<ol start="3">
<li>
<p><strong>Submit Pipeline:</strong>
<code>aws emr add-steps --cluster-id j-xxxxx \ --steps '[{ &quot;Name&quot;: &quot;E2E-Pipeline&quot;, &quot;ActionOnFailure&quot;: &quot;TERMINATE_CLUSTER&quot;, &quot;HadoopJarStep&quot;: { &quot;Jar&quot;: &quot;command-runner.jar&quot;, &quot;Args&quot;: [ &quot;spark-submit&quot;, &quot;--executor-memory&quot;, &quot;2g&quot;, &quot;--num-executors&quot;, &quot;20&quot;, &quot;s3://your-bucket/scripts/end-to-end-pipeline.py&quot; ] } }]'</code></p>
</li>
<li>
<p><strong>Monitor Complete Workflow:</strong></p>
</li>
</ol>
<ul>
<li>Cluster starts with 3 instances</li>
<li>Job begins, memory usage increases</li>
<li>Auto-scaling kicks in, adds 4-6 instances</li>
<li>Processing completes</li>
<li>Scale-in begins, reduces to 3 instances</li>
<li>Job finishes, cluster can terminate</li>
</ul>
<h3 id="expected-timeline">Expected Timeline</h3>
<ul>
<li><strong>0-5 min</strong>: Job startup, initial processing</li>
<li><strong>5-10 min</strong>: Heavy load, scaling out to 8-10 instances</li>
<li><strong>10-25 min</strong>: Processing with full capacity</li>
<li><strong>25-30 min</strong>: Job completion, scaling in</li>
<li><strong>30-35 min</strong>: Final cleanup, cluster ready for next job</li>
</ul>
<h2 id="performance-metrics-analysis">Performance Metrics Analysis</h2>
<h3 id="key-performance-indicators-kpis">Key Performance Indicators (KPIs)</h3>
<p><strong>Cost Efficiency:</strong></p>
<ul>
<li>Cost per GB processed</li>
<li>Cost per job completion</li>
<li>Utilization percentage</li>
</ul>
<p><strong>Performance:</strong></p>
<ul>
<li>Job completion time</li>
<li>Throughput (GB/hour)</li>
<li>Resource efficiency</li>
</ul>
<p><strong>Reliability:</strong></p>
<ul>
<li>Success rate</li>
<li>Spot interruption impact</li>
<li>Recovery time</li>
</ul>
<h3 id="benchmarking-results">Benchmarking Results</h3>
<p><strong>Static Cluster (Baseline):</strong></p>
<ul>
<li>10 instances × 2 hours = 20 instance-hours</li>
<li>Cost: $1.92</li>
<li>Processing time: 45 minutes</li>
<li>Utilization: 60%
<strong>Auto-Scaling Cluster:</strong></li>
<li>Average 6 instances × 2 hours = 12 instance-hours</li>
<li>Cost: $1.15</li>
<li>Processing time: 50 minutes</li>
<li>Utilization: 85%</li>
</ul>
<p><strong>Improvement:</strong></p>
<ul>
<li>40% cost reduction</li>
<li>25% better utilization</li>
<li>Only 11% longer processing time</li>
</ul>
<h2 id="graduation-exercise">Graduation Exercise</h2>
<h3 id="challenge-optimize-real-workload">Challenge: Optimize Real Workload</h3>
<p>You are provided with a 10GB dataset and requirements:</p>
<ol>
<li><strong>Process data with maximum budget of $2</strong></li>
<li><strong>Complete within 60 minutes</strong></li>
<li><strong>Achieve 80%+ cluster utilization</strong></li>
<li><strong>Handle at least 1 spot interruption</strong></li>
</ol>
<h3 id="solution-approach">Solution Approach</h3>
<ol>
<li>
<p><strong>Cluster Configuration:</strong>
<code>json { &quot;InstanceGroups&quot;: [ { &quot;Name&quot;: &quot;Master&quot;, &quot;InstanceRole&quot;: &quot;MASTER&quot;,  &quot;InstanceType&quot;: &quot;m5.large&quot;, &quot;InstanceCount&quot;: 1, &quot;Market&quot;: &quot;ON_DEMAND&quot; }, { &quot;Name&quot;: &quot;Core&quot;, &quot;InstanceRole&quot;: &quot;CORE&quot;, &quot;InstanceType&quot;: &quot;m5.large&quot;,  &quot;InstanceCount&quot;: 1, &quot;Market&quot;: &quot;ON_DEMAND&quot; }, { &quot;Name&quot;: &quot;Task&quot;, &quot;InstanceRole&quot;: &quot;TASK&quot;, &quot;InstanceType&quot;: &quot;m5.large&quot;, &quot;InstanceCount&quot;: 0, &quot;Market&quot;: &quot;SPOT&quot;, &quot;BidPrice&quot;: &quot;0.04&quot; } ] } </code></p>
</li>
<li>
<p><strong>Scaling Policy:</strong>
<code>json { &quot;ComputeLimits&quot;: { &quot;UnitType&quot;: &quot;Instances&quot;, &quot;MinimumCapacityUnits&quot;: 2, &quot;MaximumCapacityUnits&quot;: 12, &quot;MaximumOnDemandCapacityUnits&quot;: 2, &quot;MaximumCoreCapacityUnits&quot;: 2 } } </code></p>
</li>
<li>
<p><strong>Optimized Spark Job:</strong>
``python</p>
</li>
</ol>
<h1 id="optimized-processingpy">optimized-processing.py</h1>
<p>from pyspark.sql import SparkSession
from pyspark.sql.functions import *</p>
<p>spark = SparkSession.builder <br>
.appName(&ldquo;OptimizedProcessing&rdquo;) <br>
.config(&ldquo;spark.sql.adaptive.enabled&rdquo;, &ldquo;true&rdquo;) <br>
.config(&ldquo;spark.sql.adaptive.coalescePartitions.enabled&rdquo;, &ldquo;true&rdquo;) <br>
.config(&ldquo;spark.serializer&rdquo;, &ldquo;org.apache.spark.serializer.KryoSerializer&rdquo;) <br>
.getOrCreate()</p>
<h1 id="efficient-data-reading">Efficient data reading</h1>
<p>df = spark.read.parquet(&ldquo;s3://your-bucket/data/&rdquo;) <br>
.repartition(100)  # Optimal partitioning</p>
<h1 id="checkpoint-to-handle-spot-interruptions">Checkpoint to handle spot interruptions</h1>
<p>spark.sparkContext.setCheckpointDir(&ldquo;s3://your-bucket/checkpoints/&rdquo;)
df.checkpoint()</p>
<h1 id="processing-with-caching-strategy">Processing with caching strategy</h1>
<p>df_processed = df.groupBy(&ldquo;category&rdquo;, &ldquo;date&rdquo;) <br>
.agg(
sum(&ldquo;amount&rdquo;).alias(&ldquo;total_amount&rdquo;),
count(&quot;*&quot;).alias(&ldquo;transaction_count&rdquo;),
avg(&ldquo;amount&rdquo;).alias(&ldquo;avg_amount&rdquo;)
) <br>
.cache()</p>
<h1 id="multiple-outputs-to-maximize-resource-usage">Multiple outputs to maximize resource usage</h1>
<p>df_processed.write.mode(&ldquo;overwrite&rdquo;) <br>
.partitionBy(&ldquo;date&rdquo;) <br>
.parquet(&ldquo;s3://your-bucket/output/summary/&rdquo;)</p>
<p>df_processed.filter(col(&ldquo;total_amount&rdquo;) &gt; 1000) <br>
.write.mode(&ldquo;overwrite&rdquo;) <br>
.json(&ldquo;s3://your-bucket/output/high-value/&rdquo;)</p>
<p>spark.stop()
``</p>
<h3 id="success-criteria-validation">Success Criteria Validation</h3>
<p><strong>Cost Check:</strong>
<code>aws ce get-cost-and-usage \ --time-period Start=2024-01-01,End=2024-01-02 \ --granularity DAILY \ --metrics BlendedCost \ --group-by Type=DIMENSION,Key=SERVICE</code></p>
<p><strong>Performance Check:</strong></p>
<ul>
<li>Monitor job completion time</li>
<li>Check cluster utilization metrics</li>
<li>Verify data processing accuracy</li>
</ul>
<p><strong>Reliability Check:</strong></p>
<ul>
<li>Simulate spot interruption</li>
<li>Verify job recovery</li>
<li>Check data consistency</li>
</ul>
<h2 id="advanced-auto-scaling-techniques">Advanced Auto Scaling Techniques</h2>
<h3 id="1-predictive-scaling-with-machine-learning">1. Predictive Scaling with Machine Learning</h3>
<p>``python</p>
<h1 id="predictive-scalingpy">predictive-scaling.py</h1>
<p>import boto3
import pandas as pd
from sklearn.linear_model import LinearRegression
import numpy as np</p>
<p>class PredictiveScaler:
def <strong>init</strong>(self, cluster_id):
self.cluster_id = cluster_id
self.cloudwatch = boto3.client(&lsquo;cloudwatch&rsquo;)
self.emr = boto3.client(&rsquo;emr&rsquo;)</p>
<pre><code>def get_historical_metrics(self, days=30):
    # Get historical YARN metrics
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days)
    
    response = self.cloudwatch.get_metric_statistics(
        Namespace='AWS/ElasticMapReduce',
        MetricName='YARNMemoryAvailablePercentage',
        Dimensions=[
            {'Name': 'JobFlowId', 'Value': self.cluster_id}
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,
        Statistics=['Average']
    )
    
    return response['Datapoints']

def predict_scaling_needs(self):
    # Simple ML model for prediction
    data = self.get_historical_metrics()
    df = pd.DataFrame(data)
    
    if len(df) &lt; 24:  # Need at least 24 hours of data
        return None
    
    # Feature engineering
    df['hour'] = df['Timestamp'].dt.hour
    df['day_of_week'] = df['Timestamp'].dt.dayofweek
    
    # Train model
    X = df[['hour', 'day_of_week']]
    y = df['Average']
    
    model = LinearRegression()
    model.fit(X, y)
    
    # Predict next hour
    next_hour = datetime.now().hour + 1
    next_day = datetime.now().weekday()
    
    predicted_usage = model.predict([[next_hour, next_day]])[0]
    
    # Recommend scaling action
    if predicted_usage &lt; 20:  # Low memory available
        return 'scale_out'
    elif predicted_usage &gt; 80:  # High memory available
        return 'scale_in'
    else:
        return 'no_action'

def apply_predictive_scaling(self):
    action = self.predict_scaling_needs()
    
    if action == 'scale_out':
        # Pre-emptively scale out
        self.emr.put_managed_scaling_policy(
            ClusterId=self.cluster_id,
            ManagedScalingPolicy={
                'ComputeLimits': {
                    'UnitType': 'Instances',
                    'MinimumCapacityUnits': 4,  # Increase minimum
                    'MaximumCapacityUnits': 15
                }
            }
        )
    elif action == 'scale_in':
        # Allow more aggressive scale-in
        self.emr.put_managed_scaling_policy(
            ClusterId=self.cluster_id,
            ManagedScalingPolicy={
                'ComputeLimits': {
                    'UnitType': 'Instances',
                    'MinimumCapacityUnits': 2,  # Reduce minimum
                    'MaximumCapacityUnits': 10
                }
            }
        )
</code></pre>
<h1 id="usage-1">Usage</h1>
<p>scaler = PredictiveScaler(&lsquo;j-xxxxx&rsquo;)
scaler.apply_predictive_scaling()
``</p>
<h3 id="2-multi-cluster-auto-scaling">2. Multi-Cluster Auto Scaling</h3>
<p>For very large workloads, manage multiple clusters:</p>
<p>``python</p>
<h1 id="multi-cluster-managerpy">multi-cluster-manager.py</h1>
<p>import boto3
import json
from datetime import datetime</p>
<p>class MultiClusterManager:
def <strong>init</strong>(self):
self.emr = boto3.client(&rsquo;emr&rsquo;)
self.cloudwatch = boto3.client(&lsquo;cloudwatch&rsquo;)
self.max_clusters = 3
self.cluster_template = {
&lsquo;Name&rsquo;: &lsquo;Auto-Cluster&rsquo;,
&lsquo;ReleaseLabel&rsquo;: &rsquo;emr-6.15.0&rsquo;,
&lsquo;Applications&rsquo;: [{&lsquo;Name&rsquo;: &lsquo;Spark&rsquo;}],
&lsquo;ServiceRole&rsquo;: &lsquo;EMR_DefaultRole&rsquo;,
&lsquo;JobFlowRole&rsquo;: &lsquo;EMR_EC2_DefaultRole&rsquo;
}</p>
<pre><code>def get_active_clusters(self):
    response = self.emr.list_clusters(
        ClusterStates=['STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING']
    )
    return [cluster for cluster in response['Clusters'] 
            if 'Auto-Cluster' in cluster['Name']]

def get_cluster_load(self, cluster_id):
    try:
        response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ElasticMapReduce',
            MetricName='ContainerPending',
            Dimensions=[{'Name': 'JobFlowId', 'Value': cluster_id}],
            StartTime=datetime.now() - timedelta(minutes=10),
            EndTime=datetime.now(),
            Period=300,
            Statistics=['Average']
        )
        
        if response['Datapoints']:
            return response['Datapoints'][-1]['Average']
        return 0
    except:
        return 0

def should_create_new_cluster(self):
    active_clusters = self.get_active_clusters()
    
    if len(active_clusters) &gt;= self.max_clusters:
        return False
    
    # Check if all clusters are heavily loaded
    total_pending = 0
    for cluster in active_clusters:
        pending = self.get_cluster_load(cluster['Id'])
        total_pending += pending
    
    # Create new cluster if average pending &gt; 10
    avg_pending = total_pending / len(active_clusters) if active_clusters else 0
    return avg_pending &gt; 10

def create_cluster(self):
    response = self.emr.run_job_flow(
        **self.cluster_template,
        Instances={
            'MasterInstanceType': 'm5.large',
            'SlaveInstanceType': 'm5.large',
            'InstanceCount': 3,
            'Ec2KeyName': 'your-key',
            'InstanceGroups': [
                {
                    'Name': 'Master',
                    'InstanceRole': 'MASTER',
                    'InstanceType': 'm5.large',
                    'InstanceCount': 1,
                    'Market': 'ON_DEMAND'
                },
                {
                    'Name': 'Core',
                    'InstanceRole': 'CORE',
                    'InstanceType': 'm5.large',
                    'InstanceCount': 2,
                    'Market': 'SPOT',
                    'BidPrice': '0.05'
                }
            ]
        }
    )
    
    cluster_id = response['JobFlowId']
    
    # Enable auto-scaling on new cluster
    self.emr.put_managed_scaling_policy(
        ClusterId=cluster_id,
        ManagedScalingPolicy={
            'ComputeLimits': {
                'UnitType': 'Instances',
                'MinimumCapacityUnits': 3,
                'MaximumCapacityUnits': 10
            }
        }
    )
    
    return cluster_id

def terminate_idle_clusters(self):
    active_clusters = self.get_active_clusters()
    
    for cluster in active_clusters:
        # Check if cluster is idle
        steps = self.emr.list_steps(
            ClusterId=cluster['Id'],
            StepStates=['RUNNING', 'PENDING']
        )
        
        if not steps['Steps']:
            # No running steps, check idle time
            cluster_details = self.emr.describe_cluster(ClusterId=cluster['Id'])
            ready_time = cluster_details['Cluster']['Status']['Timeline'].get('ReadyDateTime')
            
            if ready_time:
                idle_minutes = (datetime.now() - ready_time).total_seconds() / 60
                if idle_minutes &gt; 30:  # Idle for 30 minutes
                    self.emr.terminate_job_flows(JobFlowIds=[cluster['Id']])
                    print(f&quot;Terminated idle cluster: {cluster['Id']}&quot;)

def manage_clusters(self):
    # Create new cluster if needed
    if self.should_create_new_cluster():
        new_cluster = self.create_cluster()
        print(f&quot;Created new cluster: {new_cluster}&quot;)
    
    # Terminate idle clusters
    self.terminate_idle_clusters()
</code></pre>
<h1 id="schedule-this-to-run-every-5-minutes">Schedule this to run every 5 minutes</h1>
<p>manager = MultiClusterManager()
manager.manage_clusters()
``</p>
<h3 id="3-cost-aware-scaling">3. Cost-Aware Scaling</h3>
<p>Implement scaling that considers cost constraints:</p>
<p>``python</p>
<h1 id="cost-aware-scalingpy">cost-aware-scaling.py</h1>
<p>import boto3
from datetime import datetime, timedelta</p>
<p>class CostAwareScaler:
def <strong>init</strong>(self, cluster_id, daily_budget=50):
self.cluster_id = cluster_id
self.daily_budget = daily_budget
self.emr = boto3.client(&rsquo;emr&rsquo;)
self.ce = boto3.client(&lsquo;ce&rsquo;)
self.cloudwatch = boto3.client(&lsquo;cloudwatch&rsquo;)</p>
<pre><code>def get_current_spend(self):
    today = datetime.now().strftime('%Y-%m-%d')
    
    response = self.ce.get_cost_and_usage(
        TimePeriod={
            'Start': today,
            'End': today
        },
        Granularity='DAILY',
        Metrics=['BlendedCost'],
        GroupBy=[
            {'Type': 'DIMENSION', 'Key': 'SERVICE'}
        ]
    )
    
    emr_cost = 0
    for result in response['ResultsByTime']:
        for group in result['Groups']:
            if 'ElasticMapReduce' in group['Keys'][0]:
                emr_cost += float(group['Metrics']['BlendedCost']['Amount'])
    
    return emr_cost

def calculate_hourly_cost(self):
    instances = self.emr.list_instances(
        ClusterId=self.cluster_id,
        InstanceStates=['RUNNING']
    )
    
    hourly_cost = 0
    for instance in instances['Instances']:
        instance_type = instance['InstanceType']
        market = instance.get('Market', 'ON_DEMAND')
        
        # Simplified cost calculation
        if 'm5.large' in instance_type:
            cost = 0.096 if market == 'ON_DEMAND' else 0.035
        elif 'm5.xlarge' in instance_type:
            cost = 0.192 if market == 'ON_DEMAND' else 0.070
        else:
            cost = 0.1
        
        hourly_cost += cost
    
    return hourly_cost

def can_scale_out(self, additional_instances=1):
    current_spend = self.get_current_spend()
    hourly_cost = self.calculate_hourly_cost()
    
    # Estimate cost for additional instances
def can_scale_out(self, additional_instances=1):
    current_spend = self.get_current_spend()
    hourly_cost = self.calculate_hourly_cost()
    
    # Estimate cost for additional instances
    additional_cost = additional_instances * 0.035  # Spot price
    hours_remaining = 24 - datetime.now().hour
    
    projected_spend = current_spend + (hourly_cost + additional_cost) * hours_remaining
    
    return projected_spend &lt;= self.daily_budget * 0.9  # 90% of budget

def smart_scale_decision(self):
    # Get current metrics
    response = self.cloudwatch.get_metric_statistics(
        Namespace='AWS/ElasticMapReduce',
        MetricName='ContainerPending',
        Dimensions=[{'Name': 'JobFlowId', 'Value': self.cluster_id}],
        StartTime=datetime.now() - timedelta(minutes=10),
        EndTime=datetime.now(),
        Period=300,
        Statistics=['Average']
    )
    
    pending_containers = 0
    if response['Datapoints']:
        pending_containers = response['Datapoints'][-1]['Average']
    
    # Scale out decision
    if pending_containers &gt; 5 and self.can_scale_out():
        return 'scale_out'
    elif pending_containers == 0:
        return 'scale_in'
    else:
        return 'no_action'

def apply_cost_aware_scaling(self):
    decision = self.smart_scale_decision()
    current_spend = self.get_current_spend()
    budget_used = (current_spend / self.daily_budget) * 100
    
    print(f&quot;Budget used: {budget_used:.1f}%&quot;)
    print(f&quot;Scaling decision: {decision}&quot;)
    
    if decision == 'scale_out' and budget_used &lt; 80:
        # Conservative scaling when budget is tight
        max_capacity = 15 if budget_used &lt; 50 else 8
        
        self.emr.put_managed_scaling_policy(
            ClusterId=self.cluster_id,
            ManagedScalingPolicy={
                'ComputeLimits': {
                    'UnitType': 'Instances',
                    'MinimumCapacityUnits': 3,
                    'MaximumCapacityUnits': max_capacity
                }
            }
        )
    elif decision == 'scale_in' or budget_used &gt; 90:
        # Aggressive scale-in when over budget
        self.emr.put_managed_scaling_policy(
            ClusterId=self.cluster_id,
            ManagedScalingPolicy={
                'ComputeLimits': {
                    'UnitType': 'Instances',
                    'MinimumCapacityUnits': 2,
                    'MaximumCapacityUnits': 5
                }
            }
        )
</code></pre>
<h1 id="usage-2">Usage</h1>
<p>scaler = CostAwareScaler(&lsquo;j-xxxxx&rsquo;, daily_budget=100)
scaler.apply_cost_aware_scaling()
``</p>
<h2 id="production-deployment-checklist">Production Deployment Checklist</h2>
<h3 id="pre-deployment">Pre-Deployment</h3>
<ul>
<li><input disabled="" type="checkbox"> IAM roles configured with proper permissions</li>
<li><input disabled="" type="checkbox"> VPC and security groups set up</li>
<li><input disabled="" type="checkbox"> S3 buckets created for data and logs</li>
<li><input disabled="" type="checkbox"> CloudWatch alarms configured</li>
<li><input disabled="" type="checkbox"> SNS topics for notifications</li>
<li><input disabled="" type="checkbox"> Lambda functions for automation</li>
</ul>
<h3 id="scaling-configuration">Scaling Configuration</h3>
<ul>
<li><input disabled="" type="checkbox"> Minimum capacity set to handle baseline load</li>
<li><input disabled="" type="checkbox"> Maximum capacity set within budget constraints</li>
<li><input disabled="" type="checkbox"> Cooldown periods optimized for workload</li>
<li><input disabled="" type="checkbox"> Instance types diversified for availability</li>
<li><input disabled="" type="checkbox"> Spot/On-Demand mix configured</li>
</ul>
<h3 id="monitoring-setup">Monitoring Setup</h3>
<ul>
<li><input disabled="" type="checkbox"> CloudWatch dashboard created</li>
<li><input disabled="" type="checkbox"> Cost tracking enabled</li>
<li><input disabled="" type="checkbox"> Performance metrics monitored</li>
<li><input disabled="" type="checkbox"> Alert thresholds configured</li>
<li><input disabled="" type="checkbox"> Log aggregation set up</li>
</ul>
<h3 id="testing-validation">Testing Validation</h3>
<ul>
<li><input disabled="" type="checkbox"> Scale-out tested with heavy workload</li>
<li><input disabled="" type="checkbox"> Scale-in tested with light workload</li>
<li><input disabled="" type="checkbox"> Spot interruption handling verified</li>
<li><input disabled="" type="checkbox"> Cost limits enforced</li>
<li><input disabled="" type="checkbox"> Performance benchmarks established</li>
</ul>
<h2 id="conclusion-part-2">Conclusion Part 2</h2>
<h3 id="what-youve-accomplished">What You&rsquo;ve Accomplished:</h3>
<ul>
<li>✅ <strong>EMR Managed Scaling</strong>: Set up and configured successfully</li>
<li>✅ <strong>Cost Optimization</strong>: Achieved 40% cost reduction with dynamic scaling</li>
<li>✅ <strong>Performance Tuning</strong>: Optimized Spark for auto-scaling environment</li>
<li>✅ <strong>Monitoring</strong>: Set up scaling events and metrics tracking</li>
<li>✅ <strong>Troubleshooting</strong>: Learned to handle common scaling issues</li>
<li>✅ <strong>Production Patterns</strong>: Implemented best practices for real-world usage</li>
</ul>
<h3 id="cost-savings-summary">Cost Savings Summary:</h3>
<ul>
<li><strong>Part 1 (Spot)</strong>: 39% savings</li>
<li><strong>Part 2 (Auto Scaling)</strong>: 40% additional savings</li>
<li><strong>Combined</strong>: ~65% total cost reduction compared to static On-Demand cluster</li>
</ul>
<h3 id="key-takeaways">Key Takeaways:</h3>
<ol>
<li><strong>Auto-scaling works best with</strong>: Predictable workload patterns, proper monitoring, and cost constraints</li>
<li><strong>Spot + Auto-scaling combination</strong>: Provides maximum cost efficiency</li>
<li><strong>Monitoring is critical</strong>: For troubleshooting and optimization</li>
<li><strong>Application design matters</strong>: Stateless, fault-tolerant applications scale better</li>
</ol>
<h3 id="real-world-impact">Real-World Impact:</h3>
<ul>
<li><strong>Small company (10 clusters)</strong>: $2,000/month → $700/month = $1,300 saved</li>
<li><strong>Medium company (50 clusters)</strong>: $10,000/month → $3,500/month = $6,500 saved</li>
<li><strong>Large enterprise (200 clusters)</strong>: $40,000/month → $14,000/month = $26,000 saved</li>
</ul>
<h3 id="next-steps">Next Steps:</h3>
<p>In Part 3, we&rsquo;ll set up comprehensive monitoring and alerting to ensure your optimized clusters run smoothly and cost-effectively in production.</p>

<div class="notices tip" ><p><strong>Pro Tip</strong>: In production, combine auto-scaling with scheduled scaling for predictable workloads to achieve maximum 70-80% cost savings!</p>
</div>

<hr>
<p><strong>Next:</strong> <a href="/03-monitoring">Part 3: Monitoring &amp; Alerting</a> to complete your cost optimization journey.</p>





<footer class=" footline" >
    
</footer>

        
        </div> 
        

      </div>

    <div id="navigation">
        
        
        
        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
        
        


	 
	 
		
			<a class="nav nav-prev" href="/000058-SessionManager/1-spot-instances/" title="Part 1: Spot Instances"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="/000058-SessionManager/3-mornitoring/" title="Part 3: Monitoring &amp; Alerting" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>
    
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="/000058-SessionManager/js/clipboard.min.js?1755036724"></script>
    <script src="/000058-SessionManager/js/perfect-scrollbar.min.js?1755036724"></script>
    <script src="/000058-SessionManager/js/perfect-scrollbar.jquery.min.js?1755036724"></script>
    <script src="/000058-SessionManager/js/jquery.sticky.js?1755036724"></script>
    <script src="/000058-SessionManager/js/featherlight.min.js?1755036724"></script>
    <script src="/000058-SessionManager/js/highlight.pack.js?1755036724"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="/000058-SessionManager/js/modernizr.custom-3.6.0.js?1755036724"></script>
    <script src="/000058-SessionManager/js/learn.js?1755036724"></script>
    <script src="/000058-SessionManager/js/hugo-learn.js?1755036724"></script>

    <link href="/000058-SessionManager/mermaid/mermaid.css?1755036724" rel="stylesheet" />
    <script src="/000058-SessionManager/mermaid/mermaid.js?1755036724"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new ();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-158079754-2', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>
